{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p3BQeqhpwC7"
   },
   "source": [
    "# Archeo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFEAYIshp9ly"
   },
   "source": [
    "## 1. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxq9RgqHqEDX"
   },
   "source": [
    "### 1.1 Loading of paths and libraries, as well as the model configuration skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in ./casini_paper-env/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.6 in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.24.6)\n",
      "Requirement already satisfied: pillow in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (10.4.0)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: six in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (1.16.0)\n",
      "Requirement already satisfied: timm==0.9.7 in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.9.7)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (0.19.0)\n",
      "Requirement already satisfied: tqdm in ./casini_paper-env/lib/python3.12/site-packages (from segmentation_models_pytorch) (4.66.5)\n",
      "Requirement already satisfied: torch in ./casini_paper-env/lib/python3.12/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.4.0)\n",
      "Requirement already satisfied: munch in ./casini_paper-env/lib/python3.12/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in ./casini_paper-env/lib/python3.12/site-packages (from timm==0.9.7->segmentation_models_pytorch) (6.0.2)\n",
      "Requirement already satisfied: safetensors in ./casini_paper-env/lib/python3.12/site-packages (from timm==0.9.7->segmentation_models_pytorch) (0.4.4)\n",
      "Requirement already satisfied: filelock in ./casini_paper-env/lib/python3.12/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./casini_paper-env/lib/python3.12/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./casini_paper-env/lib/python3.12/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (24.1)\n",
      "Requirement already satisfied: requests in ./casini_paper-env/lib/python3.12/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./casini_paper-env/lib/python3.12/site-packages (from huggingface-hub>=0.24.6->segmentation_models_pytorch) (4.12.2)\n",
      "Requirement already satisfied: numpy in ./casini_paper-env/lib/python3.12/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.4)\n",
      "Requirement already satisfied: sympy in ./casini_paper-env/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./casini_paper-env/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./casini_paper-env/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in ./casini_paper-env/lib/python3.12/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (73.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./casini_paper-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./casini_paper-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./casini_paper-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./casini_paper-env/lib/python3.12/site-packages (from requests->huggingface-hub>=0.24.6->segmentation_models_pytorch) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./casini_paper-env/lib/python3.12/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./casini_paper-env/lib/python3.12/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ZrWVv32S8I_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in ./casini_paper-env/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in ./casini_paper-env/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.6.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (0.11.7)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (24.1)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (2.4.0)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (1.4.0.post0)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: pytorch-lightning in ./casini_paper-env/lib/python3.12/site-packages (from lightning) (2.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./casini_paper-env/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10.5)\n",
      "Requirement already satisfied: setuptools in ./casini_paper-env/lib/python3.12/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (73.0.1)\n",
      "Requirement already satisfied: filelock in ./casini_paper-env/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.15.4)\n",
      "Requirement already satisfied: sympy in ./casini_paper-env/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./casini_paper-env/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./casini_paper-env/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: numpy>1.20.0 in ./casini_paper-env/lib/python3.12/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./casini_paper-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./casini_paper-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./casini_paper-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./casini_paper-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./casini_paper-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./casini_paper-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./casini_paper-env/lib/python3.12/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./casini_paper-env/lib/python3.12/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./casini_paper-env/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/casini_paper-env/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "#trying to use old version of lightning to get it working and will update later.\n",
    "!pip install lightning==1.9\n",
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rich\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import pytorch\n",
    "\n",
    "import lightning \n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar \n",
    "from lightning.pytorch.callbacks import RichProgressBar\n",
    "\n",
    "import torchvision.ops\n",
    "import segmentation_models_pytorch  as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6ZrWVv32S8I_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11e8b9910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LOG= os.path.join(os.getcwd(),'exp_logs/')\n",
    "PATH_DATASETS=os.path.join(os.getcwd(),'datasets/')\n",
    "\n",
    "modelli=['bing_1k','bing_1k_filtrato','bing+corona_1k','bing+corona_2k_filtrato',\n",
    "         'bing_2k','bing_2k_filtrato']\n",
    "testset_modelli={}\n",
    "\n",
    "config = {\n",
    "    \"timestamp\" : datetime.now().strftime(\"%d-%m-%Y_%H%M%S\"),\n",
    "    \"dataset_path\" : \"\",\n",
    "    \"checkpoint_path\":PATH_LOG,\n",
    "    \"random_seed\" : 1234,\n",
    "    \"arch\":\"MAnet\", #Unet,MAnet\n",
    "    \"encoder\":\"efficientnet-b3\", #resnet18, dpn68, efficientnet-b3\n",
    "    \"weights\":\"imagenet\",\n",
    "    \"loss\":\"focal\",\n",
    "    \"learning_rate\":0.0001,\n",
    "    \"precision\":32,\n",
    "    \"epochs\":5,       #was 20\n",
    "    \"batch_size\":32,\n",
    "    \"corona_path\":\"\",\n",
    "    \"dim_input\":'',\n",
    "    \"in_channels\":0\n",
    "}\n",
    "\n",
    "random.seed(config[\"random_seed\"])\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "torch.manual_seed(config[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LFnjlL2qP8d"
   },
   "source": [
    "### 1.2 Division of the dataset into train set (80%), validation set (10%) and test set (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oxsYae5BYFpj"
   },
   "outputs": [],
   "source": [
    "def load_dataset(PATH,SEED,indices):\n",
    "    root_directory = os.path.join(PATH)\n",
    "    images_directory = os.path.join(root_directory, \"train/sites\")\n",
    "    masks_directory = os.path.join(root_directory, \"train/masks\")\n",
    "    \n",
    "    filenames_train = np.asarray(list(sorted(os.listdir(images_directory))))\n",
    "    print(\"total files:\",len(filenames_train))\n",
    "\n",
    "    valid_split = -int(len(indices)*0.2)\n",
    "    test_split = valid_split//2\n",
    "    \n",
    "    train_indices = indices[:valid_split]\n",
    "    valid_indices = indices[valid_split:test_split]\n",
    "    test_indices = indices[test_split:]\n",
    "    train_images_filenames = filenames_train[train_indices]\n",
    "    val_images_filenames = filenames_train[valid_indices]\n",
    "    test_images_filenames = filenames_train[test_indices]\n",
    "\n",
    "    print(\"root:\",root_directory,\"\\nimages\",images_directory,\n",
    "          \"\\nmasks\",masks_directory,\"\\n---\",\n",
    "          '\\ntrain images',len(train_images_filenames), \n",
    "          '\\nval images',len(val_images_filenames), \n",
    "          '\\ntest images',len(test_images_filenames),\n",
    "          '\\n---\\ntotal images',len(filenames_train)\n",
    "         )\n",
    "    \n",
    "    print(\"empty masks percentage: %.4f %.4f %.4f\" % \n",
    "          (np.sum([i.startswith(\"neg\") for i in train_images_filenames])/len(train_images_filenames),\n",
    "            np.sum([i.startswith(\"neg\") for i in val_images_filenames])/len(val_images_filenames),\n",
    "            np.sum([i.startswith(\"neg\") for i in test_images_filenames])/len(test_images_filenames)\n",
    "          ))\n",
    "    \n",
    "    return images_directory, masks_directory, train_images_filenames, val_images_filenames,test_images_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop and resize depend on input size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M3UX56KZM6KI"
   },
   "outputs": [],
   "source": [
    "def transform_images():\n",
    "  if(config[\"dim_input\"]=='1k' and config['corona_path']!=\"\"): #same crop for input bing and input corona\n",
    "    train_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  \n",
    "\n",
    "    val_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  elif(config[\"dim_input\"]=='1k' and config['corona_path']==\"\"):\n",
    "        train_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25)\n",
    "            ,A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ])\n",
    "     \n",
    "        val_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "  elif(config[\"dim_input\"]=='2k' and config['corona_path']!=\"\"):\n",
    "    train_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25)\n",
    "            ,A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  \n",
    "    val_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  else:\n",
    "    train_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25)\n",
    "            ,A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ])\n",
    "  \n",
    "    val_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ])\n",
    "  return train_transform,val_transform\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfgisvZxqlWQ"
   },
   "source": [
    "###  1.3 Loading the dataset with its transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset with its transformations, based on the type of model we are analysing,\n",
    "\n",
    "#### based on the input size of each image and the input channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vIEBqKiVYlLd"
   },
   "outputs": [],
   "source": [
    "class ArcheoDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image_path = os.path.join(self.images_directory, image_filename)\n",
    "        mask_path = os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\"))\n",
    "        \n",
    "        image = np.array(Image.open(image_path))#.convert(\"RGB\"))\n",
    "        image = Image.open(image_path)\n",
    "      \n",
    "        \n",
    "        mask = ~np.array(Image.open(mask_path).convert(\"L\")) # masks are flipped because of qgis\n",
    "        mask = mask.astype(\"float\")\n",
    "        mask[mask > 0.0] = 1.0\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "\n",
    "\n",
    "        if(config['corona_path']!=\"\"):\n",
    "          path_corona=config['corona_path']\n",
    "          image_path_corona=os.path.join(path_corona, image_filename)\n",
    "          image_corona =np.array(Image.open(image_path_corona))\n",
    "          image_corona = Image.open(image_path_corona)\n",
    "          transformed = self.transform(image=np.asarray(image),\n",
    "                                       image_corona=np.asarray(image_corona),\n",
    "                                       mask=np.asarray(mask))\n",
    "          image_corona=transformed[\"image_corona\"]\n",
    "          image = transformed[\"image\"]\n",
    "          image=torch.cat((image, image_corona), 0)\n",
    "      \n",
    "        else:\n",
    "          transformed = self.transform(image=np.asarray(image),mask=np.asarray(mask))\n",
    "          image = transformed[\"image\"]\n",
    "\n",
    "        \n",
    "        mask = transformed[\"mask\"].permute(2,0,1)\n",
    "        \n",
    "            \n",
    "        return image, mask, image_filename\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwtR-1gPpd19"
   },
   "source": [
    "### 1.4 Initialising the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model initialisation, calculation of loss function on masks, probabilities obtained \n",
    "with sigmoid. Probabilities greater than 0.5 are transformed into 1, those below 0.5 into 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XS145rxzbrY6"
   },
   "outputs": [],
   "source": [
    "class ArcheoModel(lightning.LightningModule):\n",
    "\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.training_step_outputs = []\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "        \n",
    "        if config[\"loss\"] == \"jaccard\":\n",
    "            self.loss_fn = smp.losses.JaccardLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        if config[\"loss\"] == \"dice\":\n",
    "            self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        if config[\"loss\"] == \"focal\":\n",
    "            self.loss_fn = smp.losses.FocalLoss(mode=smp.losses.BINARY_MODE)\n",
    "        \n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        if(config['corona_path']!=''):\n",
    "          image1,image2=torch.tensor_split(image, 2, dim=1)\n",
    "          image1 = (image1 - self.mean) / self.std\n",
    "          image2 = (image2 - self.mean) / self.std\n",
    "          image=torch.cat((image1, image2), 1)\n",
    "        else:\n",
    "          image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[0]\n",
    "        assert image.ndim == 4\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "        mask = batch[1]\n",
    "        assert mask.ndim == 4\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "        logits_mask = self.forward(image)\n",
    "        #calculate loss\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "        #Logging loss to a dictionery\n",
    "        self.log_dict({f\"{stage}/loss\": loss.detach().item()},batch_size=config[\"batch_size\"])\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()  \n",
    "        pred_mask = pred_mask.permute(0,3,1,2)\n",
    "        mask = mask.permute(0,3,1,2)\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
    "\n",
    "        self.training_step_outputs.append([tp,fp,fn,tn])\n",
    "        \n",
    "        if stage == \"train\":\n",
    "            self.log_dict({\n",
    "                \"train/batch-IOU-img\":smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro-imagewise\"),\n",
    "                \"train/batch-IOU\":smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "            }, prog_bar=True, batch_size=config[\"batch_size\"])\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro-imagewise\")\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}/IOU-img\": per_image_iou,\n",
    "            f\"{stage}/IOU\": dataset_iou,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True,batch_size=config[\"batch_size\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")          \n",
    "\n",
    "    #def training_epoch_end(self, outputs): #Deprecated below is new method\n",
    "        #return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    ##New method\n",
    "    #def on_train_epoch_end(self):\n",
    "        #epoch_average = torch.stack\n",
    "\n",
    "    ## New Method\n",
    "    #def on_train_batch_end(self, outputs):\n",
    "       # self.train_loss_metric(outputs['loss'])\n",
    "       # self.log(\n",
    "       #     \"finetuning/train_loss\",\n",
    "       #     self.train_loss_metric,\n",
    "       #     prog_bar=self.prog_bar,\n",
    "       #     on_step=False,\n",
    "       #     on_epoch=True\n",
    "       # )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "    \n",
    "    #def validation_epoch_end(self, outputs):\n",
    "        #return self.shared_epoch_end(outputs, \"valid\")\n",
    "    \n",
    "    #def on_validation_epoch_end(self, outputs):\n",
    "        #return self.shared_epoch_end(outputs, \"valid\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=config[\"learning_rate\"]) # 0.00005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-xl-quxvE3Li"
   },
   "outputs": [],
   "source": [
    "def set_config(name_modello):\n",
    "  config[\"dataset_path\"]=PATH_DATASETS+name_modello\n",
    "  if '1k' in name_modello:\n",
    "    config[\"dim_input\"]='1k'\n",
    "    if('+' in name_modello):\n",
    "      config[\"dataset_path\"]=PATH_DATASETS+'bing_1k/'\n",
    "      config['corona_path']=PATH_DATASETS+'corona_1k/train/sites/'\n",
    "      config['in_channels']=6\n",
    "    else:\n",
    "      config['corona_path']=\"\"\n",
    "      config['in_channels']=3\n",
    "  else:\n",
    "    config[\"dim_input\"]='2k'\n",
    "    if('+' in name_modello):\n",
    "      config['corona_path']=PATH_DATASETS+'corona_2k_filtrato/train/sites/'\n",
    "      config['in_channels']=6\n",
    "      config[\"dataset_path\"]=PATH_DATASETS+'bing_2k_filtrato/'\n",
    "    else:\n",
    "      config['corona_path']=\"\"\n",
    "      config['in_channels']=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gorxvbn7rFnL"
   },
   "source": [
    "### 1.5 Training of each model according to its characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5peABn__byW7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files: 4734\n",
      "[1933  972  173 ... 2128 1130 3216]\n",
      "total files: 4734\n",
      "root: /Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/datasets/bing_1k \n",
      "images /Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/datasets/bing_1k/train/sites \n",
      "masks /Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/datasets/bing_1k/train/masks \n",
      "--- \n",
      "train images 3788 \n",
      "val images 473 \n",
      "test images 473 \n",
      "---\n",
      "total images 4734\n",
      "empty masks percentage: 0.0000 0.0000 0.0000\n",
      "timestamp : **10-10-2024_203249**  \n",
      "dataset_path : **/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/datasets/bing_1k**  \n",
      "checkpoint_path : **/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/exp_logs/**  \n",
      "random_seed : **1234**  \n",
      "arch : **MAnet**  \n",
      "encoder : **efficientnet-b3**  \n",
      "weights : **imagenet**  \n",
      "loss : **focal**  \n",
      "learning_rate : **0.0001**  \n",
      "precision : **32**  \n",
      "epochs : **5**  \n",
      "batch_size : **32**  \n",
      "corona_path : ****  \n",
      "dim_input : **1k**  \n",
      "in_channels : **3**  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model   │ MAnet     │ 17.2 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ loss_fn │ FocalLoss │      0 │ train │\n",
       "└───┴─────────┴───────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model   │ MAnet     │ 17.2 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ loss_fn │ FocalLoss │      0 │ train │\n",
       "└───┴─────────┴───────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 17.2 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 17.2 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 68                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 539                                                                                         \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 17.2 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 17.2 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 68                                                                         \n",
       "\u001b[1mModules in train mode\u001b[0m: 539                                                                                         \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1430b7ec171e4cda98565140e98e1d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/casini_paper-env/lib/python3.12/site-packages/light\n",
       "ning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may \n",
       "be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the \n",
       "`DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/casini_paper-env/lib/python3.12/site-packages/light\n",
       "ning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may \n",
       "be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the \n",
       "`DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/datasets/bing_1k_filtrato/train/sites'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(modelli)):\n\u001b[1;32m      2\u001b[0m  set_config(modelli[i])\n\u001b[0;32m----> 3\u001b[0m  filenames_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain/sites\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal files:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(filenames_train))\n\u001b[1;32m      5\u001b[0m  indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(filenames_train))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hatton/Nextcloud/my_files/code/adapted_tell_segmentation/datasets/bing_1k_filtrato/train/sites'"
     ]
    }
   ],
   "source": [
    " for i in range(len(modelli)):\n",
    "  set_config(modelli[i])\n",
    "  filenames_train = np.asarray(list(sorted(os.listdir(os.path.join(config[\"dataset_path\"], \"train/sites\")))))\n",
    "  print(\"total files:\",len(filenames_train))\n",
    "  indices = np.arange(0,len(filenames_train))\n",
    "  np.random.shuffle(indices)\n",
    "  print(indices)\n",
    "  \n",
    "\n",
    "  images_directory, masks_directory, train_images_filenames,val_images_filenames,test_images_filenames = load_dataset(config[\"dataset_path\"],config[\"random_seed\"], indices)\n",
    "  train_transform,val_transform=transform_images()\n",
    "  train_dataset = ArcheoDataset(train_images_filenames, images_directory, \n",
    "                                masks_directory,transform=train_transform)\n",
    "  val_dataset = ArcheoDataset(val_images_filenames, images_directory, \n",
    "                              masks_directory, transform=val_transform)\n",
    "  train_loader = DataLoader(train_dataset,batch_size=config[\"batch_size\"], \n",
    "                            shuffle=True,drop_last=True,num_workers=0,)\n",
    "  val_loader = DataLoader(val_dataset,batch_size=config[\"batch_size\"], \n",
    "                          shuffle=False,drop_last=False,num_workers=0,)\n",
    "  model = ArcheoModel(config[\"arch\"],encoder_name=config[\"encoder\"],\n",
    "                      encoder_weights=config[\"weights\"],\n",
    "                      in_channels=config['in_channels'], \n",
    "                      out_classes=1,) #creazione del modello\n",
    "\n",
    "#TRAINING DEL MODELLO\n",
    "\n",
    "  trainer = lightning.Trainer(\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    precision=config[\"precision\"],\n",
    "    accelerator=\"cpu\",\n",
    "    logger=pl_loggers.TensorBoardLogger(config[\"checkpoint_path\"]),\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[RichProgressBar(refresh_rate=1)],\n",
    "   \n",
    "  )\n",
    "  cfg_text = \"\\n\".join([ str(key)+\" : **\"+str(config[key])+\"**  \" for key in config])\n",
    "  print(cfg_text)\n",
    "  trainer.logger.experiment.add_text(tag=\"config\",text_string=cfg_text)\n",
    "\n",
    "  trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=val_loader)\n",
    "  os.rename(PATH_LOG+'lightning_logs/version_0', PATH_LOG+'lightning_logs/'+modelli[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THEwd9-GrQwQ"
   },
   "source": [
    "## 2 Testing of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NTsZfXkFdiEr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c735df5ef7697fb9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c735df5ef7697fb9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=PATH_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pYBPlPw9BTOj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files: 4734\n",
      "[4066 1623 1403 ... 3970  278 1539]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/exp_logs/lightning_logs/bing_1k/checkpoints/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(indices)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices)\n\u001b[0;32m---> 15\u001b[0m name_ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_LOG\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlightning_logs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodelli\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/checkpoints/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m ArcheoModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(arch\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m                encoder_name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     18\u001b[0m                encoder_weights\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m                in_channels\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     20\u001b[0m                out_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m                checkpoint_path\u001b[38;5;241m=\u001b[39mPATH_LOG\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mmodelli[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/checkpoints/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mname_ckpt)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/exp_logs/lightning_logs/bing_1k/checkpoints/'"
     ]
    }
   ],
   "source": [
    "#10 tests, with different transformations, for each model. \n",
    "iou_test_modelli={}\n",
    "for i in range(len(modelli)):\n",
    "  set_config(modelli[i])\n",
    " \n",
    "  filenames_train = np.asarray(list(sorted(os.listdir(\n",
    "      os.path.join(config[\"dataset_path\"], \"train/sites\")))))\n",
    "  print(\"total files:\",len(filenames_train))\n",
    "  indices = np.arange(0,len(filenames_train))\n",
    "  np.random.shuffle(indices)\n",
    "  print(indices)\n",
    "  \n",
    "  \n",
    " \n",
    "  name_ckpt = os.listdir(PATH_LOG+'lightning_logs/'+modelli[i]+'/checkpoints/')[0]\n",
    "  model = ArcheoModel.load_from_checkpoint(arch=config[\"arch\"],\n",
    "                 encoder_name=config[\"encoder\"], \n",
    "                 encoder_weights=config[\"weights\"],\n",
    "                 in_channels=config['in_channels'], \n",
    "                 out_classes=1,\n",
    "                 checkpoint_path=PATH_LOG+'lightning_logs/'+modelli[i]+'/checkpoints/'+name_ckpt)\n",
    "  print(config['dataset_path'])\n",
    "  images_directory, masks_directory, train_images_filenames,\n",
    "  val_images_filenames,test_images_filenames = load_dataset(config[\"dataset_path\"],\n",
    "                                                            config[\"random_seed\"],indices)\n",
    "  \n",
    "  testiou=[]\n",
    "  for j in range(10):\n",
    "    print(\"Risultato \"+str(j)+\" del modello \"+modelli[i])\n",
    "    train_transform,val_transform=transform_images()\n",
    "    test_dataset = ArcheoDataset(test_images_filenames, images_directory, \n",
    "                                 masks_directory, transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=config[\"batch_size\"],\n",
    "                             shuffle=False,drop_last=False,num_workers=0,)\n",
    "\n",
    "    trainer = L.Trainer(gpus=1, max_epochs=40,auto_scale_batch_size=\"binsearch\",\n",
    "                         precision=16,accelerator=\"auto\",)\n",
    " \n",
    "    test_metrics = trainer.test(model, dataloaders=test_loader, verbose=True)\n",
    "    testiou.append(test_metrics[0]['test/IOU-img'])\n",
    "  iou_test_modelli[modelli[i]]=testiou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tm0RObDA_BPA",
    "outputId": "cb33e9a8-7d60-453c-caab-dafaeeebc366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le statistiche sul test set per il modello bing_1k sono:\n",
      "media:0.7417 | min:0.7356 | max:0.7468 | std:0.0038\n",
      "Le statistiche sul test set per il modello bing_1k_filtrato sono:\n",
      "media:0.781 | min:0.7689 | max:0.788 | std:0.0054\n",
      "Le statistiche sul test set per il modello bing+corona_1k sono:\n",
      "media:0.7406 | min:0.7347 | max:0.746 | std:0.0039\n",
      "Le statistiche sul test set per il modello bing+corona_2k_filtrato sono:\n",
      "media:0.8345 | min:0.8312 | max:0.8376 | std:0.0018\n",
      "Le statistiche sul test set per il modello bing_2k sono:\n",
      "media:0.7977 | min:0.7929 | max:0.8031 | std:0.0034\n",
      "Le statistiche sul test set per il modello bing_2k_filtrato sono:\n",
      "media:0.8154 | min:0.8087 | max:0.8223 | std:0.0035\n"
     ]
    }
   ],
   "source": [
    "#Stampo le statistiche per ogni modello (mean,min,max,std)\n",
    "for i in modelli:\n",
    "  x=np.array(iou_test_modelli[i])\n",
    "  print(\"Le statistiche sul test set per il modello \"+i+\" sono:\")\n",
    "  print(\"media:\"+str(round(x.mean(),4))+\" | min:\"+str(round(x.min(),4))\n",
    "    +\" | max:\"+str(round(x.max(),4))+\" | std:\"+str(round(x.std(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxyJKE2ZVQEW"
   },
   "source": [
    "##3 Valutazione Modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDsztoQgZnBL"
   },
   "source": [
    "### 3.1 Caricamento dei csv contenenti coordinate siti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0We15_RvVWOS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "namesite2Centroid={}\n",
    "nameneg2Centroid={}\n",
    "namemaysan2Centroid={}\n",
    "name2min={}\n",
    "\n",
    "\n",
    "with open('trainset1000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "      namesite2Centroid[row[2]]=[row[3],row[4]]\n",
    "      \n",
    "\n",
    "with open('negs1000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        nameneg2Centroid[row[5]]=[row[3],row[4]]\n",
    "\n",
    "\n",
    "with open('maysan1000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        namemaysan2Centroid[row[1]]=[row[2],row[3]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ31iVC-ZvJ7"
   },
   "source": [
    "###3.2 Carico il dataset applicandogli trasformazioni prestabilite\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4tHQ6NpYgnp"
   },
   "outputs": [],
   "source": [
    "class ArcheoDatasetModify(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx, verbose=False):\n",
    "      image_filename = self.images_filenames[idx]+'.jpg'\n",
    "      image_path = os.path.join(self.images_directory, image_filename)\n",
    "      mask_path = os.path.join(self.masks_directory, \n",
    "                               image_filename.replace(\".jpg\", \".png\"))\n",
    "        \n",
    "      image = np.array(Image.open(image_path))#.convert(\"RGB\"))\n",
    "      image = Image.open(image_path)\n",
    "      # masks are flipped because of qgis\n",
    "      mask = ~np.array(Image.open(mask_path).convert(\"L\")) \n",
    "      mask = mask.astype(\"float\")\n",
    "      mask[mask > 0.0] = 1.0\n",
    "      mask = np.expand_dims(mask, -1)\n",
    "      x_minore=int(name2tras[image_filename[:-4]][0])\n",
    "      y_minore=int(name2tras[image_filename[:-4]][1])\n",
    "      if(config['corona_path']!=''):     \n",
    "        path_corona=config['corona_path']\n",
    "        image_path_corona=os.path.join(path_corona, image_filename)\n",
    "        image_corona =np.array(Image.open(image_path_corona))\n",
    "        image_corona = Image.open(image_path_corona)\n",
    "  \n",
    "        trasformazione = A.Compose([\n",
    "            A.Crop(x_min=x_minore, y_min=y_minore, \n",
    "                   x_max=x_minore+1024, y_max=y_minore+1024,p=1.0),\n",
    "            A.Resize(512,512),\n",
    "            A.pytorch.ToTensorV2()],additional_targets={'image_corona': 'image'})\n",
    "        transformed = trasformazione(image=np.asarray(image),\n",
    "                                     image_corona=np.asarray(image_corona),\n",
    "                                     mask=np.asarray(mask))\n",
    "        image_corona=transformed[\"image_corona\"]\n",
    "        image = transformed[\"image\"]\n",
    "        image=torch.cat((image, image_corona), 0)\n",
    "      \n",
    "      else:\n",
    "        trasformazione = A.Compose([\n",
    "            A.Crop(x_min=x_minore, y_min=y_minore, \n",
    "                   x_max=x_minore+1024, y_max=y_minore+1024,p=1.0),\n",
    "            A.Resize(512,512),\n",
    "            A.pytorch.ToTensorV2()])\n",
    "        transformed = trasformazione(image=np.asarray(image),\n",
    "                                     mask=np.asarray(mask))\n",
    "        image = transformed[\"image\"]\n",
    "\n",
    "      mask = transformed[\"mask\"].permute(2,0,1)\n",
    "      return image, mask, image_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSN2bOPoZ_1p"
   },
   "source": [
    "###3.3 Salvataggio delle singole predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvYGKQorZkSn"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#data la predizione,viene applicato un cutoff\n",
    "def stampa_predizioni_cutoff(ibatch,ipr_masks_11,num,nome_modello,val):\n",
    "    for i,(image, gt_mask, masks_11, fn) in enumerate(zip(ibatch[0], ibatch[1], \n",
    "                                                          ipr_masks_11, ibatch[2])):\n",
    "      masks_11 = masks_11.numpy().squeeze()\n",
    "      # cutoff\n",
    "      cf = masks_11.copy()\n",
    "      cf = gaussian_filter(cf, sigma=5)\n",
    "      cf = ((cf + 0.5)**2) - 0.5\n",
    "      cf[cf<=val] = 0.0\n",
    "      cf[cf>val] = 1.0\n",
    "\n",
    "\n",
    "      plt.imsave('/output/'+nome_modello+'/pred_siti_tronc'+\n",
    "                 str(val)+'/'+ fn[:-4]+'.png',cf, cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izYHcVUK2fqL"
   },
   "source": [
    "###3.4 Date le predizioni per ogni sito, creo i tif e gli shapefile corrispondenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWXX71yiiSPb"
   },
   "outputs": [],
   "source": [
    "#Dato la predizione, viene creato il tif associato. \n",
    "import rasterio\n",
    "def createTif(ovest,sud,est,nord,patin,patout):\n",
    "  dataset = rasterio.open(patin, 'r')\n",
    "  bands = [1, 2, 3]\n",
    "  data = dataset.read(bands)\n",
    "  transform = rasterio.transform.from_bounds(ovest, sud, est, nord, data.shape[2], data.shape[1])\n",
    "  crs = {'init': 'epsg:3857'}\n",
    "\n",
    "  with rasterio.open(patout, 'w', driver='GTiff',\n",
    "                   width=data.shape[2], height=data.shape[1],\n",
    "                   count=3, dtype=data.dtype, nodata=0,\n",
    "                   transform=transform, crs=crs) as dst:\n",
    "      dst.write(data, indexes=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5sQpAQAicdU"
   },
   "outputs": [],
   "source": [
    "#Ricostruisco la predizione usando le trasformazioni salvate in precedenza.\n",
    "def convertImageToTif(nome_modello,val_tronc,path_pred,path_pred_total,path_tif):\n",
    "  for i in range(len(test_images_filenames)):\n",
    "    \n",
    "    im_sfondo=Image.open('black_2048.jpg')\n",
    "    im_sito = Image.open(path_pred+test_images_filenames[i]+'.png')\n",
    "    \n",
    "    newsize = (1024, 1024)\n",
    "    im_sito = im_sito.resize(newsize)\n",
    "    x_min=int(name2tras[test_images_filenames[i]][0])\n",
    "    x_max=(int(name2tras[test_images_filenames[i]][0])+1024)\n",
    "    y_min=int(name2tras[test_images_filenames[i]][1])\n",
    "    y_max=(int(name2tras[test_images_filenames[i]][1])+1024)\n",
    "    im_sfondo.paste(im_sito, (x_min, y_min,x_max,y_max))\n",
    "    im_sfondo = im_sfondo.save(path_pred_total+test_images_filenames[i]+'.png')\n",
    "    if(\"neg\" in test_images_filenames[i]):\n",
    "      x_centroide=float(nameneg2Centroid[test_images_filenames[i]][0])\n",
    "      y_centroide=float(nameneg2Centroid[test_images_filenames[i]][1])\n",
    "    else:\n",
    "      x_centroide=float(namesite2Centroid[test_images_filenames[i]][0])\n",
    "      y_centroide=float(namesite2Centroid[test_images_filenames[i]][1])\n",
    "    ovest=x_centroide-1000\n",
    "    est=x_centroide+1000\n",
    "    sud=y_centroide-1000\n",
    "    nord=y_centroide+1000\n",
    "    createTif(ovest=ovest, sud=sud, est=est, nord=nord,\n",
    "              patin=path_pred_total+test_images_filenames[i]+'.png',\n",
    "              patout=path_tif+test_images_filenames[i]+\".tif\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjwqUkkaxPXx"
   },
   "outputs": [],
   "source": [
    "#prende i tif files e restituisce gli shape files\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "from gdal import gdal_contour\n",
    "\n",
    "def convertTifToShape(tif_path,shape_path):\n",
    "  filenames = os.listdir(tif_path)\n",
    "  print(len(filenames)) #521\n",
    "  for f in tqdm(filenames):\n",
    "    print(f[:-4])\n",
    "    subprocess.run([\"gdal_contour.exe\",\"-i\",\"128\",\"-p\",tif_path+f,shape_path+f[:-4]+\".shp\"],\n",
    "                   capture_output=True,shell=True,check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZUWOQoGl9T0"
   },
   "outputs": [],
   "source": [
    "#prende in input il file csv con le trasformazioni per ogni sito, \n",
    "#ritorna i nomi dei siti e un dizionario nome_sito -> trasformazioni\n",
    "def crea_trasformazione(path_csv):\n",
    "  test_sites=[]\n",
    "  name2min={}\n",
    "  count=0\n",
    "  with open(path_csv, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in (reader):\n",
    "        if(count!=0):\n",
    "          test_sites.append(row[1])\n",
    "          name2min[row[1]]=[row[2],row[3]]\n",
    "        count+=1\n",
    "    \n",
    "  return test_sites,name2min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0IVHb-nJXQA"
   },
   "source": [
    "###3.5 Dati gli shapefile in input e restituisco un geojson contenente per ogni sito TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFGZZCWUGr7M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as geopd\n",
    "from tqdm.auto import tqdm\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "\n",
    "#Assegna ad ogni sito tp,tn,fp,fn in base all'intersezione della forma predetta \n",
    "#con la forma originale\n",
    "def test_for_intersection(site_id,path,verbose=False):\n",
    "    ### get shape for the original site\n",
    "    sites = geopd.read_file(PATH_SHAPE).to_crs(\"EPSG:3857\") # project to web-mercator\n",
    "    a = sites[sites.entry_id == site_id][[\"entry_id\",\"geometry\"]]\n",
    "    \n",
    "    if verbose: print('Loading Contours')\n",
    "    b = geopd.read_file(path+site_id+\".shp\").set_crs(\"EPSG:3857\")\n",
    "    b.geometry = b.geometry.convex_hull\n",
    "    b[\"entry_id\"] = \"pred\"\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"number of features:\",len(b))\n",
    "        print(b)\n",
    "        \n",
    "    if len(b) > 1: # if there is more than one shape\n",
    "        b[\"geometry\"] = MultiPolygon([feature for feature in b[\"geometry\"]])\n",
    "        b = b[:1].copy()\n",
    "        if verbose: \n",
    "            print(b)\n",
    "            b.plot()\n",
    "    # negs should have no geometry\n",
    "    if len(b) == 0 and site_id.startswith(\"neg\"):\n",
    "        if verbose: print(\"good neg\")\n",
    "        return \"TN\",site_id,None\n",
    "    \n",
    "    # if neg has geometry then FP\n",
    "    elif len(b) > 0 and site_id.startswith(\"neg\"):\n",
    "        if verbose: print(\"false positive\")\n",
    "        return \"FP\",site_id,b.iloc[0][\"geometry\"] #False\n",
    "    \n",
    "    # if no geometry and not neg then FN. use a.geometry for use in QGIS\n",
    "    elif len(b) == 0 and not site_id.startswith(\"neg\"):\n",
    "        if verbose: print(\"false negative\")\n",
    "        return \"FN\",site_id, a.iloc[0][\"geometry\"]\n",
    "    \n",
    "    # compute intersection\n",
    "    if ~b.iloc[0].geometry.is_valid:\n",
    "        b.geometry = b.geometry.buffer(0)\n",
    "    intersects = a.iloc[0][\"geometry\"].intersects(b.iloc[0][\"geometry\"])\n",
    "    if verbose:\n",
    "        c = pd.concat([a,b])\n",
    "        # print(c)\n",
    "        c.plot(column=\"entry_id\",legend=True,figsize=(5,5),cmap=\"Set3\")\n",
    "        print(\"INTERSECTION: \", intersects)\n",
    "        \n",
    "    if intersects: # right geometry\n",
    "        return \"TP\",site_id,b.iloc[0][\"geometry\"]\n",
    "    else: # wrong geometry\n",
    "        return \"FP\",site_id,b.iloc[0][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqIYsriiHX9B"
   },
   "outputs": [],
   "source": [
    "import geopandas as geopd\n",
    "#prende in input gli shapefile dei siti e ritorna un geojson \n",
    "#contenente id,nome sito, geometria e valore tra tp,tn,fp,fn\n",
    "def convertShapeToGeojson(testset,path_in,path_out):\n",
    " res = {\"index\":[],\"entry_id\":[],\"geometry\":[],\"cat\":[],}\n",
    " indice=0\n",
    " for sito in testset:\n",
    "    cat,eid,geom = test_for_intersection(sito,path_in,verbose=False)\n",
    "    res[\"index\"].append(indice)\n",
    "    res[\"entry_id\"].append(eid)\n",
    "    res[\"geometry\"].append(geom)\n",
    "    res[\"cat\"].append(cat)\n",
    "    indice+=1\n",
    " res_df = geopd.GeoDataFrame(res)\n",
    " res_df = res_df.set_index(\"index\")\n",
    " res_df.to_file(path_out, driver='GeoJSON',crs=\"EPSG:3857\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmUHRD95J-ro"
   },
   "source": [
    "###3.6 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uB86uU4ZehpX"
   },
   "outputs": [],
   "source": [
    "modelli_da_comparare=['bing+corona_2k_filtrato','bing_2k_filtrato']\n",
    "PATH_SHAPE='/shapefiles/site_shape/vw_site_survey_poly.shp'\n",
    "for modello in modelli_da_comparare:\n",
    "  PATH_OUTPUT='output/'+modello+'/'\n",
    "  set_config(modello)\n",
    "  name_ckpt = os.listdir(PATH_LOG+'lightning_logs/'+modello+'/checkpoints/')[0]\n",
    "  model= ArcheoModel.load_from_checkpoint(arch=config[\"arch\"],\n",
    "                 encoder_name=config[\"encoder\"], \n",
    "                 encoder_weights=config[\"weights\"],\n",
    "                 in_channels=config['in_channels'], out_classes=1,\n",
    "                 checkpoint_path=PATH_LOG+'lightning_logs/'+modello+'/checkpoints/'+name_ckpt)\n",
    "  random.seed(config[\"random_seed\"])\n",
    "  np.random.seed(config[\"random_seed\"])\n",
    "  torch.manual_seed(config[\"random_seed\"])\n",
    "  test_images_filenames,name2tras=crea_trasformazione('trasformazioni_modello.csv')\n",
    "  images_directory=config[\"dataset_path\"]+'/train/sites'\n",
    "  masks_directory=config[\"dataset_path\"]+'/train/masks'\n",
    "  test_dataset = ArcheoDatasetModify(test_images_filenames, \n",
    "                                     images_directory, \n",
    "                                     masks_directory)\n",
    "  test_loader = DataLoader(test_dataset,batch_size=config[\"batch_size\"],\n",
    "                           shuffle=False,drop_last=False,num_workers=0,)\n",
    "\n",
    "  print(len(test_images_filenames))\n",
    "  it = iter(test_loader)\n",
    "\n",
    "\n",
    "  for j in range(len(it)):\n",
    "    batch = next(it)\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      logits = model(batch[0])\n",
    "    pr_masks = logits.sigmoid()\n",
    "    stampa_predizioni_cutoff(batch,pr_masks,j,modello,0.2)\n",
    "    stampa_predizioni_cutoff(batch,pr_masks,j,modello,0.5)\n",
    "\n",
    "  convertImageToTif(modello,0.2,PATH_OUTPUT+'pred_siti_tronc0.2/',\n",
    "                    PATH_OUTPUT+'pred_siti_centro_tronc0.2/',PATH_OUTPUT+'tif_0.2/')\n",
    "  convertImageToTif(modello,0.5,PATH_OUTPUT+'pred_siti_tronc0.5/',\n",
    "                    PATH_OUTPUT+'pred_siti_centro_tronc0.5/',PATH_OUTPUT+'tif_0.5/')\n",
    "\n",
    "  convertTifToShape(PATH_OUTPUT+'tif_0.2/',PATH_OUTPUT+'shape_0.2/')\n",
    "  convertTifToShape(PATH_OUTPUT+'tif_0.5/',PATH_OUTPUT+'shape_0.5/')\n",
    "\n",
    "  convertShapeToGeojson(test_images_filenames,PATH_OUTPUT+'shape_0.2/',PATH_OUTPUT+'preds02.geojson')\n",
    "  convertShapeToGeojson(test_images_filenames,PATH_OUTPUT+'shape_0.5/',PATH_OUTPUT+'preds05.geojson')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJaH3-WzNyMt"
   },
   "source": [
    "## 4 Analisi Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_0SyTrCU8Uj"
   },
   "source": [
    "### 4.1 Calcolo della matrice di confusione, in modo automatico e adattato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9g4FjnPJJaO"
   },
   "outputs": [],
   "source": [
    "def calcola_matrix(preds):\n",
    "  tp= preds[preds.cat == \"TP\"]['entry_id'].shape[0]\n",
    "  tn= preds[preds.cat == \"TN\"]['entry_id'].shape[0]\n",
    "  fp= preds[preds.cat == \"FP\"]['entry_id'].shape[0]\n",
    "  fn= preds[preds.cat == \"FN\"]['entry_id'].shape[0]\n",
    "\n",
    "  matrix=[tp,tn,fp,fn]\n",
    "\n",
    "  # sono siti non visibili, quindi classificati erroneamente come tn\n",
    "  fn2tn = preds[(preds.cat == \"FN\")&(preds.correction == \"TN\")]['entry_id'].shape[0]\n",
    "\n",
    "  sites_inside_ot=preds[(preds.notes.str.contains('INSIDE OTHER',na=False)) ].shape[0]\n",
    "\n",
    "  #siti non visibili e il modello ne trova un altro esistente\n",
    "  fp2tp = preds[\n",
    "    (preds.cat == \"FP\")& ((preds.notes.str.contains('NV',na=False))| \n",
    "                        (preds.notes.str.contains('NOT VISIBLE',na=False)) |\n",
    "                        (preds.notes.str.contains('NOT VISIBILE',na=False)) |\n",
    "                        (preds.entry_id.str.contains('neg',na=False)))&\n",
    "                        (preds.notes.str.contains('INSIDE OTHER',na=False))].shape[0]\n",
    "\n",
    "  #siti visibili e il modello ne trova un altro esistente \n",
    "  fp2fn=sites_inside_ot-fp2tp\n",
    "\n",
    "  tn_a = tn + fn2tn + fp2tp\n",
    "  fn_a = fn - fn2tn + fp2fn\n",
    "  tp_a = tp + (fp2tp+fp2fn)\n",
    "  fp_a = fp - (fp2tp+fp2fn)\n",
    "\n",
    "  matrix_adj=[tp_a,tn_a,fp_a,fn_a]\n",
    "  \n",
    "  return(matrix,matrix_adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_DrUrJ0ZsGy"
   },
   "outputs": [],
   "source": [
    "#mi stampo le statistiche in termini di valori della matrice di confusione oltre a\n",
    "# precisione,recall.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def stampa_stats(cm,cm_adj,bing):\n",
    "  print('--------------------------------------------')\n",
    "  if(bing):\n",
    "    print(\"Stats Modello Bing 2k filtrato:\")\n",
    "  else:\n",
    "    print(\"Stats Modello Bing + Corona 2k filtrato:\")\n",
    "\n",
    "  print('---')\n",
    "\n",
    "  print(\"Valutazione automatica:\")\n",
    "  print(\"TP: \"+str(cm[0])+\" TN: \"+str(cm[1])+\" FP: \"+str(cm[2])+\" FN: \"+str(cm[3]))\n",
    "  print(\"Accuracy: \",round((cm[0]+cm[1])/(cm[0]+cm[1]+cm[2]+cm[3]),4))\n",
    "  print(\"Recall: \",round(cm[0]/(cm[0]+cm[3]),4))\n",
    "\n",
    "  print('---')\n",
    "\n",
    "  print(\"Valutazione manuale:\")\n",
    "  print(\"TP: \"+str(cm_adj[0])+\" TN: \"+str(cm_adj[1])+\" FP: \"+str(cm_adj[2])+\" FN: \"+str(cm_adj[3]))\n",
    "  print(\"Accuracy: \",round((cm_adj[0]+cm_adj[1])/(cm_adj[0]+cm_adj[1]+cm_adj[2]+cm_adj[3]),4))\n",
    "  print(\"Recall: \",round(cm_adj[0]/(cm_adj[0]+cm_adj[3]),4))\n",
    "\n",
    "  print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_GDMmUXk-q5",
    "outputId": "391f0ee3-9a46-490b-ff09-fa8f7a7c5a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Stats Modello Bing 2k filtrato:\n",
      "---\n",
      "Valutazione automatica:\n",
      "TP: 228 TN: 98 FP: 70 FN: 125\n",
      "Accuracy:  0.6257\n",
      "Recall:  0.6459\n",
      "---\n",
      "Valutazione manuale:\n",
      "TP: 258 TN: 185 FP: 40 FN: 68\n",
      "Accuracy:  0.804\n",
      "Recall:  0.7914\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Stats Modello Bing + Corona 2k filtrato:\n",
      "---\n",
      "Valutazione automatica:\n",
      "TP: 209 TN: 104 FP: 57 FN: 151\n",
      "Accuracy:  0.6008\n",
      "Recall:  0.5806\n",
      "---\n",
      "Valutazione manuale:\n",
      "TP: 239 TN: 197 FP: 27 FN: 88\n",
      "Accuracy:  0.7913\n",
      "Recall:  0.7309\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import geopandas as geopd\n",
    "PATH_GEOJSON_BING='/output/bing_2k_filtrato/preds05.geojson'\n",
    "PATH_GEOJSON_CORONA='/output/bing+corona_2k_filtrato/preds05.geojson'\n",
    "preds_bing = geopd.read_file(PATH_GEOJSON_BING).sort_values(\"index\").reset_index(drop=True)\n",
    "preds_corona = geopd.read_file(PATH_GEOJSON_CORONA).sort_values(\"index\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "cm_bing,cm_bing_adj=calcola_matrix(preds_bing)\n",
    "stampa_stats(cm_bing,cm_bing_adj,bing=True)\n",
    "\n",
    "\n",
    "cm_corona,cm_corona_adj=calcola_matrix(preds_corona)\n",
    "stampa_stats(cm_corona,cm_corona_adj,bing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OVuUTDUGFpo"
   },
   "source": [
    "##5 Area di selezione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckmrJv7FV3jQ"
   },
   "source": [
    "###5.1 Crea le predizioni per ogni tessera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAwuOOJuMSdR"
   },
   "outputs": [],
   "source": [
    "PATH_MAYSAN_DATASET='/datasets/maysan_sel_tile/'\n",
    "PATH_MAYSAN_OUTPUT='/output/maysan_sel_area/'\n",
    "PATH_MODEL_USED=PATH_LOG+'lightning_logs/bing+corona_2k_filtrato/checkpoints/epoch=19-step=2600.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ymw6DzvxvQXH"
   },
   "outputs": [],
   "source": [
    "def stampa_predizioni(ibatch,ipr_masks_11,num,patout):\n",
    "  for i,(image, gt_mask, masks_11, fn) in enumerate(zip(ibatch[0], ibatch[1], ipr_masks_11, ibatch[2])):\n",
    "\n",
    "    masks_11 = masks_11.numpy().squeeze()\n",
    "    plt.imsave(patout+'pred_magma_v1/'+ fn[:-4]+'.png',masks_11,cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "    plt.imsave(patout+'pred_grey_v1/'+ fn[:-4]+'.png',masks_11,cmap=\"Greys\", vmin=0.0, vmax=1.0)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0agFYumlGUnM"
   },
   "outputs": [],
   "source": [
    "filenames_test_maysan = np.asarray(list(sorted(os.listdir(PATH_MAYSAN_DATASET+'sites'))))\n",
    "config['corona_path']=PATH_MAYSAN_DATASET+'corona_v1/'\n",
    "transform_maysan=A.Compose([A.Resize(512, 512),A.pytorch.ToTensorV2(),],\n",
    "                           additional_targets={'image_corona': 'image'})\n",
    "test_dataset_maysan = ArcheoDataset(filenames_test_maysan, PATH_MAYSAN_DATASET+'sites/', \n",
    "                                    PATH_MAYSAN_DATASET+'masks/', transform=transform_maysan)\n",
    "test_loader_maysan = DataLoader(test_dataset_maysan,batch_size=config[\"batch_size\"], \n",
    "                                shuffle=False,drop_last=False,num_workers=0,)\n",
    "model = ArcheoModel.load_from_checkpoint(arch=config[\"arch\"],\n",
    "                 encoder_name=config[\"encoder\"], \n",
    "                 encoder_weights=config[\"weights\"],\n",
    "                 in_channels=6, out_classes=1,checkpoint_path=PATH_MODEL_USED)\n",
    "\n",
    "random.seed(config[\"random_seed\"])\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "torch.manual_seed(config[\"random_seed\"])\n",
    "\n",
    "it_maysan=iter(test_loader_maysan)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "for i in range(len(it_maysan)):\n",
    "  batch = next(it_maysan)\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(batch[0])\n",
    "  pr_masks = logits.sigmoid()\n",
    "  stampa_predizioni(batch,pr_masks,i,PATH_MAYSAN_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjqZMMcmWAnx"
   },
   "source": [
    "###5.2 Assembla l'immagine totale dell'area di selezione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq2DYnpepMBm"
   },
   "outputs": [],
   "source": [
    "def crea_righe(imm_size,l_x,l_y,fn,resize=False,num_res=256,format=\".png\"):\n",
    "  righe=[]\n",
    "  altezza=1\n",
    "  if(resize==True):\n",
    "    image_tot = Image.open(fn+\"1\"+format).resize((num_res,num_res))\n",
    "  else:\n",
    "    image_tot = Image.open(fn+\"1\"+format)\n",
    "  for i in range(2,l_x*l_y+1):\n",
    "    if((i-1)%l_x==0):\n",
    "      if(resize==True):\n",
    "        image_tot=Image.open(fn+str(i)+format).resize((num_res,num_res))\n",
    "      else:\n",
    "        image_tot=Image.open(fn+str(i)+format)\n",
    "    else:\n",
    "      if(resize==True):\n",
    "        image_open=Image.open(fn+str(i)+format).resize((num_res,num_res))\n",
    "      else:\n",
    "        image_open=Image.open(fn+str(i)+format)\n",
    "      new_image = Image.new('RGB',(imm_size*l_x, imm_size))\n",
    "      new_image.paste(image_tot,(0,0))\n",
    "      posizione=i%l_x\n",
    "      if(posizione==0):\n",
    "        posizione=l_x\n",
    "      new_image.paste(image_open,(imm_size*(posizione-1),((altezza-1)*image_open.size[1])))\n",
    "      image_tot=new_image\n",
    "      if(posizione==l_x):righe.append(image_tot)\n",
    "  return righe\n",
    "\n",
    "def crea_immagine(righ,dim,l_x,l_y,nome):\n",
    "  imm_totale = Image.new('RGB',(l_x*dim, l_y*dim))\n",
    "  x=-dim\n",
    "  for i in reversed(range(len(righ))):\n",
    "    x+=dim\n",
    "    imm_totale.paste(righ[i],(0,x))\n",
    "  imm_totale.save(PATH_MAYSAN_OUTPUT+nome+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmCZ89aHsdBQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "f = open('coor_maysan_1000.json')\n",
    "data = json.load(f)\n",
    "spost=data[1]['cx']-data[0]['cx']\n",
    "ovest=int(data[0]['cx']-spost/2)\n",
    "sud=int(data[0]['cy']-spost/2)\n",
    "nord=int(data[len(data)-1]['cy']+(spost/2))\n",
    "num_righe=int((nord-sud)/spost)\n",
    "num_colonne=int(len(data)/num_righe)\n",
    "est=int(data[num_colonne-1]['cx']+spost/2)\n",
    "\n",
    "\n",
    "righe=crea_righe(512,num_colonne,num_righe,PATH_MAYSAN_OUTPUT+'pred_magma_v1/')\n",
    "righe1=crea_righe(512,num_colonne,num_righe,PATH_MAYSAN_OUTPUT+'pred_grey_v1/')\n",
    "crea_immagine(righe,512,num_colonne,num_righe,\"imm_tot_magma_v1\")\n",
    "crea_immagine(righe1,512,num_colonne,num_righe,\"imm_tot_grey_v1\")\n",
    "createTif(ovest,sud,est,nord,PATH_MAYSAN_OUTPUT+\"imm_tot_magma_v1.jpg\",\n",
    "          PATH_MAYSAN_OUTPUT+\"imm_tot_magma_v1.tif\")\n",
    "createTif(ovest,sud,est,nord,PATH_MAYSAN_OUTPUT+\"imm_tot_grey_v1.jpg\",\n",
    "          PATH_MAYSAN_OUTPUT+\"imm_tot_grey_v1.tif\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
