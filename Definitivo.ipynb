{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p3BQeqhpwC7"
   },
   "source": [
    "# Archeo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFEAYIshp9ly"
   },
   "source": [
    "## 1. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxq9RgqHqEDX"
   },
   "source": [
    "### 1.1 Loading of paths and libraries, as well as the model configuration skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.15.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: timm==0.9.2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from segmentation_models_pytorch) (0.9.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from segmentation_models_pytorch) (4.66.1)\n",
      "Requirement already satisfied: pillow in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from segmentation_models_pytorch) (9.4.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.0.1)\n",
      "Requirement already satisfied: munch in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.26.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install segmentation_models_pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ZrWVv32S8I_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.12 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rich\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations import pytorch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, RichProgressBar\n",
    "\n",
    "import segmentation_models_pytorch  as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6ZrWVv32S8I_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x132c539f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_LOG= os.path.join(os.getcwd(),'exp_logs/')\n",
    "PATH_DATASETS=os.path.join(os.getcwd(),'datasets/')\n",
    "\n",
    "modelli=['bing_1k','bing_1k_filtrato','bing+corona_1k','bing+corona_2k_filtrato',\n",
    "         'bing_2k','bing_2k_filtrato']\n",
    "testset_modelli={}\n",
    "\n",
    "config = {\n",
    "    \"timestamp\" : datetime.now().strftime(\"%d-%m-%Y_%H%M%S\"),\n",
    "    \"dataset_path\" : \"\",\n",
    "    \"checkpoint_path\":PATH_LOG,\n",
    "    \"random_seed\" : 1234,\n",
    "    \"arch\":\"MAnet\", #Unet,MAnet\n",
    "    \"encoder\":\"efficientnet-b3\", #resnet18, dpn68, efficientnet-b3\n",
    "    \"weights\":\"imagenet\",\n",
    "    \"loss\":\"focal\",\n",
    "    \"learning_rate\":0.0001,\n",
    "    \"precision\":32,\n",
    "    \"epochs\":20,\n",
    "    \"batch_size\":32,\n",
    "    \"corona_path\":\"\",\n",
    "    \"dim_input\":'',\n",
    "    \"in_channels\":0\n",
    "}\n",
    "\n",
    "random.seed(config[\"random_seed\"])\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "torch.manual_seed(config[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LFnjlL2qP8d"
   },
   "source": [
    "### 1.2 Division of the dataset into train set (80%), validation set (10%) and test set (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oxsYae5BYFpj"
   },
   "outputs": [],
   "source": [
    "def load_dataset(PATH,SEED,indices):\n",
    "    root_directory = os.path.join(PATH)\n",
    "    images_directory = os.path.join(root_directory, \"train/sites\")\n",
    "    masks_directory = os.path.join(root_directory, \"train/masks\")\n",
    "    \n",
    "    filenames_train = np.asarray(list(sorted(os.listdir(images_directory))))\n",
    "    print(\"total files:\",len(filenames_train))\n",
    "\n",
    "    valid_split = -int(len(indices)*0.2)\n",
    "    test_split = valid_split//2\n",
    "    \n",
    "    train_indices = indices[:valid_split]\n",
    "    valid_indices = indices[valid_split:test_split]\n",
    "    test_indices = indices[test_split:]\n",
    "    train_images_filenames = filenames_train[train_indices]\n",
    "    val_images_filenames = filenames_train[valid_indices]\n",
    "    test_images_filenames = filenames_train[test_indices]\n",
    "\n",
    "    print(\"root:\",root_directory,\"\\nimages\",images_directory,\n",
    "          \"\\nmasks\",masks_directory,\"\\n---\",\n",
    "          '\\ntrain images',len(train_images_filenames), \n",
    "          '\\nval images',len(val_images_filenames), \n",
    "          '\\ntest images',len(test_images_filenames),\n",
    "          '\\n---\\ntotal images',len(filenames_train)\n",
    "         )\n",
    "    \n",
    "    print(\"empty masks percentage: %.4f %.4f %.4f\" % \n",
    "          (np.sum([i.startswith(\"neg\") for i in train_images_filenames])/len(train_images_filenames),\n",
    "            np.sum([i.startswith(\"neg\") for i in val_images_filenames])/len(val_images_filenames),\n",
    "            np.sum([i.startswith(\"neg\") for i in test_images_filenames])/len(test_images_filenames)\n",
    "          ))\n",
    "    \n",
    "    return images_directory, masks_directory, train_images_filenames, val_images_filenames,test_images_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop and resize depend on input size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M3UX56KZM6KI"
   },
   "outputs": [],
   "source": [
    "def transform_images():\n",
    "  if(config[\"dim_input\"]=='1k' and config['corona_path']!=\"\"): #same crop for input bing and input corona\n",
    "    train_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  \n",
    "\n",
    "    val_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  elif(config[\"dim_input\"]=='1k' and config['corona_path']==\"\"):\n",
    "        train_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25)\n",
    "            ,A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ])\n",
    "     \n",
    "        val_transform = A.Compose([\n",
    "            A.RandomCrop(512,512,p=1.0),\n",
    "            A.Resize(256, 256),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "  elif(config[\"dim_input\"]=='2k' and config['corona_path']!=\"\"):\n",
    "    train_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25)\n",
    "            ,A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  \n",
    "    val_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ],\n",
    "        additional_targets={'image_corona': 'image'})\n",
    "  else:\n",
    "    train_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Flip(p=0.25),A.RandomRotate90(p=0.25)\n",
    "            ,A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.25),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2(),\n",
    "        ])\n",
    "  \n",
    "    val_transform = A.Compose([\n",
    "            A.RandomCrop(1024,1024,p=1.0),\n",
    "            A.Resize(512, 512),\n",
    "            A.pytorch.ToTensorV2()\n",
    "        ])\n",
    "  return train_transform,val_transform\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfgisvZxqlWQ"
   },
   "source": [
    "### 1.3 Loading the dataset with its transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset with its transformations, based on the type of model we are analysing,\n",
    "\n",
    "#### based on the input size of each image and the input channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vIEBqKiVYlLd"
   },
   "outputs": [],
   "source": [
    "class ArcheoDataset(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory, transform=None):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.images_filenames[idx]\n",
    "        image_path = os.path.join(self.images_directory, image_filename)\n",
    "        mask_path = os.path.join(self.masks_directory, image_filename.replace(\".jpg\", \".png\"))\n",
    "        \n",
    "        image = np.array(Image.open(image_path))#.convert(\"RGB\"))\n",
    "        image = Image.open(image_path)\n",
    "      \n",
    "        \n",
    "        mask = ~np.array(Image.open(mask_path).convert(\"L\")) # masks are flipped because of qgis\n",
    "        mask = mask.astype(\"float\")\n",
    "        mask[mask > 0.0] = 1.0\n",
    "        mask = np.expand_dims(mask, -1)\n",
    "\n",
    "\n",
    "        if(config['corona_path']!=\"\"):\n",
    "          path_corona=config['corona_path']\n",
    "          image_path_corona=os.path.join(path_corona, image_filename)\n",
    "          image_corona =np.array(Image.open(image_path_corona))\n",
    "          image_corona = Image.open(image_path_corona)\n",
    "          transformed = self.transform(image=np.asarray(image),\n",
    "                                       image_corona=np.asarray(image_corona),\n",
    "                                       mask=np.asarray(mask))\n",
    "          image_corona=transformed[\"image_corona\"]\n",
    "          image = transformed[\"image\"]\n",
    "          image=torch.cat((image, image_corona), 0)\n",
    "      \n",
    "        else:\n",
    "          transformed = self.transform(image=np.asarray(image),mask=np.asarray(mask))\n",
    "          image = transformed[\"image\"]\n",
    "\n",
    "        \n",
    "        mask = transformed[\"mask\"].permute(2,0,1)\n",
    "        \n",
    "            \n",
    "        return image, mask, image_filename\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwtR-1gPpd19"
   },
   "source": [
    "### 1.4 Initialising the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model initialisation, calculation of loss function on masks, probabilities obtained \n",
    "with sigmoid. Probabilities greater than 0.5 are transformed into 1, those below 0.5 into 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XS145rxzbrY6"
   },
   "outputs": [],
   "source": [
    "class ArcheoModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arch, encoder_name, in_channels, out_classes, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "        params = smp.encoders.get_preprocessing_params(encoder_name)\n",
    "        self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n",
    "        self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n",
    "        \n",
    "        if config[\"loss\"] == \"jaccard\":\n",
    "            self.loss_fn = smp.losses.JaccardLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        if config[\"loss\"] == \"dice\":\n",
    "            self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        if config[\"loss\"] == \"focal\":\n",
    "            self.loss_fn = smp.losses.FocalLoss(mode=smp.losses.BINARY_MODE)\n",
    "        \n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        if(config['corona_path']!=''):\n",
    "          image1,image2=torch.tensor_split(image, 2, dim=1)\n",
    "          image1 = (image1 - self.mean) / self.std\n",
    "          image2 = (image2 - self.mean) / self.std\n",
    "          image=torch.cat((image1, image2), 1)\n",
    "        else:\n",
    "          image = (image - self.mean) / self.std\n",
    "        mask = self.model(image)\n",
    "        return mask\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[0]\n",
    "        assert image.ndim == 4\n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "        mask = batch[1]\n",
    "        assert mask.ndim == 4\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "        logits_mask = self.forward(image)\n",
    "        loss = self.loss_fn(logits_mask, mask)\n",
    "        self.log_dict({f\"{stage}/loss\": loss.detach().item()},batch_size=config[\"batch_size\"])\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        pred_mask = (prob_mask > 0.5).float()  \n",
    "        pred_mask = pred_mask.permute(0,3,1,2)\n",
    "        mask = mask.permute(0,3,1,2)\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
    "        \n",
    "        if stage == \"train\":\n",
    "            self.log_dict({\n",
    "                \"train/batch-IOU-img\":smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro-imagewise\"),\n",
    "                \"train/batch-IOU\":smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "            }, prog_bar=True, batch_size=config[\"batch_size\"])\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro-imagewise\")\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "\n",
    "        metrics = {\n",
    "            f\"{stage}/IOU-img\": per_image_iou,\n",
    "            f\"{stage}/IOU\": dataset_iou,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True,batch_size=config[\"batch_size\"])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")            \n",
    "\n",
    "    #def training_epoch_end(self, outputs): #Depreciated below is new method\n",
    "        #return self.shared_epoch_end(outputs, \"train\")\n",
    "    def on_train_batch_end(self, outputs):\n",
    "        self.train_loss_metric(outputs['loss'])\n",
    "        self.log(\n",
    "            \"finetuning/train_loss\",\n",
    "            self.train_loss_metric,\n",
    "            prog_bar=self.prog_bar,\n",
    "            on_step=False,\n",
    "            on_epoch=True\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=config[\"learning_rate\"]) # 0.00005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-xl-quxvE3Li"
   },
   "outputs": [],
   "source": [
    "def set_config(name_modello):\n",
    "  config[\"dataset_path\"]=PATH_DATASETS+name_modello\n",
    "  if '1k' in name_modello:\n",
    "    config[\"dim_input\"]='1k'\n",
    "    if('+' in name_modello):\n",
    "      config[\"dataset_path\"]=PATH_DATASETS+'bing_1k/'\n",
    "      config['corona_path']=PATH_DATASETS+'corona_1k/train/sites/'\n",
    "      config['in_channels']=6\n",
    "    else:\n",
    "      config['corona_path']=\"\"\n",
    "      config['in_channels']=3\n",
    "  else:\n",
    "    config[\"dim_input\"]='2k'\n",
    "    if('+' in name_modello):\n",
    "      config['corona_path']=PATH_DATASETS+'corona_2k_filtrato/train/sites/'\n",
    "      config['in_channels']=6\n",
    "      config[\"dataset_path\"]=PATH_DATASETS+'bing_2k_filtrato/'\n",
    "    else:\n",
    "      config['corona_path']=\"\"\n",
    "      config['in_channels']=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gorxvbn7rFnL"
   },
   "source": [
    "### 1.5 Training of each model according to its characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5peABn__byW7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files: 4734\n",
      "[4622  144 2391 ... 1085 4364 2716]\n",
      "total files: 4734\n",
      "root: /Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/datasets/bing_1k \n",
      "images /Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/datasets/bing_1k/train/sites \n",
      "masks /Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/datasets/bing_1k/train/masks \n",
      "--- \n",
      "train images 3788 \n",
      "val images 473 \n",
      "test images 473 \n",
      "---\n",
      "total images 4734\n",
      "empty masks percentage: 0.0000 0.0000 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (mps), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp : **29-07-2024_163231**  \n",
      "dataset_path : **/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/datasets/bing_1k**  \n",
      "checkpoint_path : **/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/exp_logs/**  \n",
      "random_seed : **1234**  \n",
      "arch : **MAnet**  \n",
      "encoder : **efficientnet-b3**  \n",
      "weights : **imagenet**  \n",
      "loss : **focal**  \n",
      "learning_rate : **0.0001**  \n",
      "precision : **32**  \n",
      "epochs : **20**  \n",
      "batch_size : **32**  \n",
      "corona_path : ****  \n",
      "dim_input : **1k**  \n",
      "in_channels : **3**  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model   │ MAnet     │ 17.2 M │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ loss_fn │ FocalLoss │      0 │ train │\n",
       "└───┴─────────┴───────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model   │ MAnet     │ 17.2 M │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ loss_fn │ FocalLoss │      0 │ train │\n",
       "└───┴─────────┴───────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 17.2 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 17.2 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 68                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 17.2 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 17.2 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 68                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5242eafb8aa47a1acb54c7884829f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connec\n",
       "tor.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the \n",
       "value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connec\n",
       "tor.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the \n",
       "value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg_text)\n\u001b[1;32m     39\u001b[0m trainer\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mexperiment\u001b[38;5;241m.\u001b[39madd_text(tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,text_string\u001b[38;5;241m=\u001b[39mcfg_text)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m  \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m os\u001b[38;5;241m.\u001b[39mrename(PATH_LOG\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs/version_0\u001b[39m\u001b[38;5;124m'\u001b[39m, PATH_LOG\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mmodelli[i])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:370\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    368\u001b[0m     batch \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mconvert_input(batch)\n\u001b[1;32m    369\u001b[0m     batch \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_on_before_batch_transfer(batch, dataloader_idx\u001b[38;5;241m=\u001b[39mdataloader_idx)\n\u001b[0;32m--> 370\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_to_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# so we need different kwargs\u001b[39;00m\n\u001b[1;32m    374\u001b[0m hook_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_kwargs(\n\u001b[1;32m    375\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_sequential \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    376\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:277\u001b[0m, in \u001b[0;36mStrategy.batch_to_device\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    275\u001b[0m device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_device\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_batch_transfer_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m move_data_to_device(batch, device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/core/module.py:358\u001b[0m, in \u001b[0;36mLightningModule._apply_batch_transfer_handler\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_batch_transfer_handler\u001b[39m(\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m, batch: Any, device: Optional[torch\u001b[38;5;241m.\u001b[39mdevice] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dataloader_idx: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    356\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    357\u001b[0m     device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 358\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransfer_batch_to_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_after_batch_transfer\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, dataloader_idx)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/core/module.py:347\u001b[0m, in \u001b[0;36mLightningModule._call_batch_hook\u001b[0;34m(self, hook_name, *args)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         trainer_method \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_lightning_datamodule_hook\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hook(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:159\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 159\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    162\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/pytorch_lightning/core/hooks.py:611\u001b[0m, in \u001b[0;36mDataHooks.transfer_batch_to_device\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransfer_batch_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Any, device: torch\u001b[38;5;241m.\u001b[39mdevice, dataloader_idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    structure.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m \n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_data_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/lightning_fabric/utilities/apply_func.py:103\u001b[0m, in \u001b[0;36mmove_data_to_device\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# user wrongly implemented the `_TransferableDataType` and forgot to return `self`.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_TransferableDataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_to\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:72\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: function(v, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# slow path for everything else\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:125\u001b[0m, in \u001b[0;36m_apply_to_collection_slow\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m--> 125\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43m_apply_to_collection_slow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrong_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrong_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_frozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_frozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_none \u001b[38;5;129;01mor\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         out\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:96\u001b[0m, in \u001b[0;36m_apply_to_collection_slow\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_to_collection_slow\u001b[39m(\n\u001b[1;32m     85\u001b[0m     data: Any,\n\u001b[1;32m     86\u001b[0m     dtype: Union[\u001b[38;5;28mtype\u001b[39m, Any, Tuple[Union[\u001b[38;5;28mtype\u001b[39m, Any]]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/lightning_fabric/utilities/apply_func.py:97\u001b[0m, in \u001b[0;36mmove_data_to_device.<locals>.batch_to\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _BLOCKING_DEVICE_TYPES:\n\u001b[1;32m     96\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_blocking\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m data_output \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_output\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    " for i in range(len(modelli)):\n",
    "  set_config(modelli[i])\n",
    "  filenames_train = np.asarray(list(sorted(os.listdir(os.path.join(config[\"dataset_path\"], \"train/sites\")))))\n",
    "  print(\"total files:\",len(filenames_train))\n",
    "  indices = np.arange(0,len(filenames_train))\n",
    "  np.random.shuffle(indices)\n",
    "  print(indices)\n",
    "  \n",
    "\n",
    "  images_directory, masks_directory, train_images_filenames,val_images_filenames,test_images_filenames = load_dataset(config[\"dataset_path\"],config[\"random_seed\"], indices)\n",
    "  train_transform,val_transform=transform_images()\n",
    "  train_dataset = ArcheoDataset(train_images_filenames, images_directory, \n",
    "                                masks_directory,transform=train_transform)\n",
    "  val_dataset = ArcheoDataset(val_images_filenames, images_directory, \n",
    "                              masks_directory, transform=val_transform)\n",
    "  train_loader = DataLoader(train_dataset,batch_size=config[\"batch_size\"], \n",
    "                            shuffle=True,drop_last=True,num_workers=0,)\n",
    "  val_loader = DataLoader(val_dataset,batch_size=config[\"batch_size\"], \n",
    "                          shuffle=False,drop_last=False,num_workers=0,)\n",
    "  model = ArcheoModel(config[\"arch\"],encoder_name=config[\"encoder\"],\n",
    "                      encoder_weights=config[\"weights\"],\n",
    "                      in_channels=config['in_channels'], \n",
    "                      out_classes=1,) #creazione del modello\n",
    "\n",
    "#TRAINING DEL MODELLO\n",
    "\n",
    "  trainer = pl.Trainer(\n",
    "    max_epochs=config[\"epochs\"],\n",
    "    precision=config[\"precision\"],\n",
    "    accelerator=\"gpu\",\n",
    "    logger=pl_loggers.TensorBoardLogger(config[\"checkpoint_path\"]),\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=True,\n",
    "    callbacks=[RichProgressBar(refresh_rate=1)],\n",
    "   \n",
    "  )\n",
    "  cfg_text = \"\\n\".join([ str(key)+\" : **\"+str(config[key])+\"**  \" for key in config])\n",
    "  print(cfg_text)\n",
    "  trainer.logger.experiment.add_text(tag=\"config\",text_string=cfg_text)\n",
    "\n",
    "  trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader, \n",
    "    val_dataloaders=val_loader)\n",
    "  os.rename(PATH_LOG+'lightning_logs/version_0', PATH_LOG+'lightning_logs/'+modelli[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THEwd9-GrQwQ"
   },
   "source": [
    "## 2 Testing of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NTsZfXkFdiEr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c735df5ef7697fb9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c735df5ef7697fb9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=PATH_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "pYBPlPw9BTOj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files: 4734\n",
      "[4066 1623 1403 ... 3970  278 1539]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/exp_logs/lightning_logs/bing_1k/checkpoints/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle(indices)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices)\n\u001b[0;32m---> 15\u001b[0m name_ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_LOG\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlightning_logs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodelli\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/checkpoints/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m ArcheoModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(arch\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124march\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m                encoder_name\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m     18\u001b[0m                encoder_weights\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m                in_channels\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_channels\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     20\u001b[0m                out_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     21\u001b[0m                checkpoint_path\u001b[38;5;241m=\u001b[39mPATH_LOG\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_logs/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mmodelli[i]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/checkpoints/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mname_ckpt)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hatton/Nextcloud/My files/code/casini_2023/adapted_tell_segmentation/exp_logs/lightning_logs/bing_1k/checkpoints/'"
     ]
    }
   ],
   "source": [
    "#10 tests, with different transformations, for each model. \n",
    "iou_test_modelli={}\n",
    "for i in range(len(modelli)):\n",
    "  set_config(modelli[i])\n",
    " \n",
    "  filenames_train = np.asarray(list(sorted(os.listdir(\n",
    "      os.path.join(config[\"dataset_path\"], \"train/sites\")))))\n",
    "  print(\"total files:\",len(filenames_train))\n",
    "  indices = np.arange(0,len(filenames_train))\n",
    "  np.random.shuffle(indices)\n",
    "  print(indices)\n",
    "  \n",
    "  \n",
    " \n",
    "  name_ckpt = os.listdir(PATH_LOG+'lightning_logs/'+modelli[i]+'/checkpoints/')[0]\n",
    "  model = ArcheoModel.load_from_checkpoint(arch=config[\"arch\"],\n",
    "                 encoder_name=config[\"encoder\"], \n",
    "                 encoder_weights=config[\"weights\"],\n",
    "                 in_channels=config['in_channels'], \n",
    "                 out_classes=1,\n",
    "                 checkpoint_path=PATH_LOG+'lightning_logs/'+modelli[i]+'/checkpoints/'+name_ckpt)\n",
    "  print(config['dataset_path'])\n",
    "  images_directory, masks_directory, train_images_filenames,\n",
    "  val_images_filenames,test_images_filenames = load_dataset(config[\"dataset_path\"],\n",
    "                                                            config[\"random_seed\"],indices)\n",
    "  \n",
    "  testiou=[]\n",
    "  for j in range(10):\n",
    "    print(\"Risultato \"+str(j)+\" del modello \"+modelli[i])\n",
    "    train_transform,val_transform=transform_images()\n",
    "    test_dataset = ArcheoDataset(test_images_filenames, images_directory, \n",
    "                                 masks_directory, transform=val_transform)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=config[\"batch_size\"],\n",
    "                             shuffle=False,drop_last=False,num_workers=0,)\n",
    "\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=40,auto_scale_batch_size=\"binsearch\",\n",
    "                         precision=16,accelerator=\"auto\",)\n",
    " \n",
    "    test_metrics = trainer.test(model, dataloaders=test_loader, verbose=True)\n",
    "    testiou.append(test_metrics[0]['test/IOU-img'])\n",
    "  iou_test_modelli[modelli[i]]=testiou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tm0RObDA_BPA",
    "outputId": "cb33e9a8-7d60-453c-caab-dafaeeebc366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le statistiche sul test set per il modello bing_1k sono:\n",
      "media:0.7417 | min:0.7356 | max:0.7468 | std:0.0038\n",
      "Le statistiche sul test set per il modello bing_1k_filtrato sono:\n",
      "media:0.781 | min:0.7689 | max:0.788 | std:0.0054\n",
      "Le statistiche sul test set per il modello bing+corona_1k sono:\n",
      "media:0.7406 | min:0.7347 | max:0.746 | std:0.0039\n",
      "Le statistiche sul test set per il modello bing+corona_2k_filtrato sono:\n",
      "media:0.8345 | min:0.8312 | max:0.8376 | std:0.0018\n",
      "Le statistiche sul test set per il modello bing_2k sono:\n",
      "media:0.7977 | min:0.7929 | max:0.8031 | std:0.0034\n",
      "Le statistiche sul test set per il modello bing_2k_filtrato sono:\n",
      "media:0.8154 | min:0.8087 | max:0.8223 | std:0.0035\n"
     ]
    }
   ],
   "source": [
    "#Stampo le statistiche per ogni modello (mean,min,max,std)\n",
    "for i in modelli:\n",
    "  x=np.array(iou_test_modelli[i])\n",
    "  print(\"Le statistiche sul test set per il modello \"+i+\" sono:\")\n",
    "  print(\"media:\"+str(round(x.mean(),4))+\" | min:\"+str(round(x.min(),4))\n",
    "    +\" | max:\"+str(round(x.max(),4))+\" | std:\"+str(round(x.std(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxyJKE2ZVQEW"
   },
   "source": [
    "##3 Valutazione Modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDsztoQgZnBL"
   },
   "source": [
    "### 3.1 Caricamento dei csv contenenti coordinate siti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0We15_RvVWOS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "namesite2Centroid={}\n",
    "nameneg2Centroid={}\n",
    "namemaysan2Centroid={}\n",
    "name2min={}\n",
    "\n",
    "\n",
    "with open('trainset1000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "      namesite2Centroid[row[2]]=[row[3],row[4]]\n",
    "      \n",
    "\n",
    "with open('negs1000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        nameneg2Centroid[row[5]]=[row[3],row[4]]\n",
    "\n",
    "\n",
    "with open('maysan1000.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        namemaysan2Centroid[row[1]]=[row[2],row[3]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ31iVC-ZvJ7"
   },
   "source": [
    "###3.2 Carico il dataset applicandogli trasformazioni prestabilite\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g4tHQ6NpYgnp"
   },
   "outputs": [],
   "source": [
    "class ArcheoDatasetModify(Dataset):\n",
    "    def __init__(self, images_filenames, images_directory, masks_directory):\n",
    "        self.images_filenames = images_filenames\n",
    "        self.images_directory = images_directory\n",
    "        self.masks_directory = masks_directory\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.images_filenames)\n",
    "\n",
    "    def __getitem__(self, idx, verbose=False):\n",
    "      image_filename = self.images_filenames[idx]+'.jpg'\n",
    "      image_path = os.path.join(self.images_directory, image_filename)\n",
    "      mask_path = os.path.join(self.masks_directory, \n",
    "                               image_filename.replace(\".jpg\", \".png\"))\n",
    "        \n",
    "      image = np.array(Image.open(image_path))#.convert(\"RGB\"))\n",
    "      image = Image.open(image_path)\n",
    "      # masks are flipped because of qgis\n",
    "      mask = ~np.array(Image.open(mask_path).convert(\"L\")) \n",
    "      mask = mask.astype(\"float\")\n",
    "      mask[mask > 0.0] = 1.0\n",
    "      mask = np.expand_dims(mask, -1)\n",
    "      x_minore=int(name2tras[image_filename[:-4]][0])\n",
    "      y_minore=int(name2tras[image_filename[:-4]][1])\n",
    "      if(config['corona_path']!=''):     \n",
    "        path_corona=config['corona_path']\n",
    "        image_path_corona=os.path.join(path_corona, image_filename)\n",
    "        image_corona =np.array(Image.open(image_path_corona))\n",
    "        image_corona = Image.open(image_path_corona)\n",
    "  \n",
    "        trasformazione = A.Compose([\n",
    "            A.Crop(x_min=x_minore, y_min=y_minore, \n",
    "                   x_max=x_minore+1024, y_max=y_minore+1024,p=1.0),\n",
    "            A.Resize(512,512),\n",
    "            A.pytorch.ToTensorV2()],additional_targets={'image_corona': 'image'})\n",
    "        transformed = trasformazione(image=np.asarray(image),\n",
    "                                     image_corona=np.asarray(image_corona),\n",
    "                                     mask=np.asarray(mask))\n",
    "        image_corona=transformed[\"image_corona\"]\n",
    "        image = transformed[\"image\"]\n",
    "        image=torch.cat((image, image_corona), 0)\n",
    "      \n",
    "      else:\n",
    "        trasformazione = A.Compose([\n",
    "            A.Crop(x_min=x_minore, y_min=y_minore, \n",
    "                   x_max=x_minore+1024, y_max=y_minore+1024,p=1.0),\n",
    "            A.Resize(512,512),\n",
    "            A.pytorch.ToTensorV2()])\n",
    "        transformed = trasformazione(image=np.asarray(image),\n",
    "                                     mask=np.asarray(mask))\n",
    "        image = transformed[\"image\"]\n",
    "\n",
    "      mask = transformed[\"mask\"].permute(2,0,1)\n",
    "      return image, mask, image_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSN2bOPoZ_1p"
   },
   "source": [
    "###3.3 Salvataggio delle singole predizioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvYGKQorZkSn"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#data la predizione,viene applicato un cutoff\n",
    "def stampa_predizioni_cutoff(ibatch,ipr_masks_11,num,nome_modello,val):\n",
    "    for i,(image, gt_mask, masks_11, fn) in enumerate(zip(ibatch[0], ibatch[1], \n",
    "                                                          ipr_masks_11, ibatch[2])):\n",
    "      masks_11 = masks_11.numpy().squeeze()\n",
    "      # cutoff\n",
    "      cf = masks_11.copy()\n",
    "      cf = gaussian_filter(cf, sigma=5)\n",
    "      cf = ((cf + 0.5)**2) - 0.5\n",
    "      cf[cf<=val] = 0.0\n",
    "      cf[cf>val] = 1.0\n",
    "\n",
    "\n",
    "      plt.imsave('/output/'+nome_modello+'/pred_siti_tronc'+\n",
    "                 str(val)+'/'+ fn[:-4]+'.png',cf, cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izYHcVUK2fqL"
   },
   "source": [
    "###3.4 Date le predizioni per ogni sito, creo i tif e gli shapefile corrispondenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWXX71yiiSPb"
   },
   "outputs": [],
   "source": [
    "#Dato la predizione, viene creato il tif associato. \n",
    "import rasterio\n",
    "def createTif(ovest,sud,est,nord,patin,patout):\n",
    "  dataset = rasterio.open(patin, 'r')\n",
    "  bands = [1, 2, 3]\n",
    "  data = dataset.read(bands)\n",
    "  transform = rasterio.transform.from_bounds(ovest, sud, est, nord, data.shape[2], data.shape[1])\n",
    "  crs = {'init': 'epsg:3857'}\n",
    "\n",
    "  with rasterio.open(patout, 'w', driver='GTiff',\n",
    "                   width=data.shape[2], height=data.shape[1],\n",
    "                   count=3, dtype=data.dtype, nodata=0,\n",
    "                   transform=transform, crs=crs) as dst:\n",
    "      dst.write(data, indexes=bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5sQpAQAicdU"
   },
   "outputs": [],
   "source": [
    "#Ricostruisco la predizione usando le trasformazioni salvate in precedenza.\n",
    "def convertImageToTif(nome_modello,val_tronc,path_pred,path_pred_total,path_tif):\n",
    "  for i in range(len(test_images_filenames)):\n",
    "    \n",
    "    im_sfondo=Image.open('black_2048.jpg')\n",
    "    im_sito = Image.open(path_pred+test_images_filenames[i]+'.png')\n",
    "    \n",
    "    newsize = (1024, 1024)\n",
    "    im_sito = im_sito.resize(newsize)\n",
    "    x_min=int(name2tras[test_images_filenames[i]][0])\n",
    "    x_max=(int(name2tras[test_images_filenames[i]][0])+1024)\n",
    "    y_min=int(name2tras[test_images_filenames[i]][1])\n",
    "    y_max=(int(name2tras[test_images_filenames[i]][1])+1024)\n",
    "    im_sfondo.paste(im_sito, (x_min, y_min,x_max,y_max))\n",
    "    im_sfondo = im_sfondo.save(path_pred_total+test_images_filenames[i]+'.png')\n",
    "    if(\"neg\" in test_images_filenames[i]):\n",
    "      x_centroide=float(nameneg2Centroid[test_images_filenames[i]][0])\n",
    "      y_centroide=float(nameneg2Centroid[test_images_filenames[i]][1])\n",
    "    else:\n",
    "      x_centroide=float(namesite2Centroid[test_images_filenames[i]][0])\n",
    "      y_centroide=float(namesite2Centroid[test_images_filenames[i]][1])\n",
    "    ovest=x_centroide-1000\n",
    "    est=x_centroide+1000\n",
    "    sud=y_centroide-1000\n",
    "    nord=y_centroide+1000\n",
    "    createTif(ovest=ovest, sud=sud, est=est, nord=nord,\n",
    "              patin=path_pred_total+test_images_filenames[i]+'.png',\n",
    "              patout=path_tif+test_images_filenames[i]+\".tif\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjwqUkkaxPXx"
   },
   "outputs": [],
   "source": [
    "#prende i tif files e restituisce gli shape files\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "from gdal import gdal_contour\n",
    "\n",
    "def convertTifToShape(tif_path,shape_path):\n",
    "  filenames = os.listdir(tif_path)\n",
    "  print(len(filenames)) #521\n",
    "  for f in tqdm(filenames):\n",
    "    print(f[:-4])\n",
    "    subprocess.run([\"gdal_contour.exe\",\"-i\",\"128\",\"-p\",tif_path+f,shape_path+f[:-4]+\".shp\"],\n",
    "                   capture_output=True,shell=True,check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZUWOQoGl9T0"
   },
   "outputs": [],
   "source": [
    "#prende in input il file csv con le trasformazioni per ogni sito, \n",
    "#ritorna i nomi dei siti e un dizionario nome_sito -> trasformazioni\n",
    "def crea_trasformazione(path_csv):\n",
    "  test_sites=[]\n",
    "  name2min={}\n",
    "  count=0\n",
    "  with open(path_csv, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in (reader):\n",
    "        if(count!=0):\n",
    "          test_sites.append(row[1])\n",
    "          name2min[row[1]]=[row[2],row[3]]\n",
    "        count+=1\n",
    "    \n",
    "  return test_sites,name2min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0IVHb-nJXQA"
   },
   "source": [
    "###3.5 Dati gli shapefile in input e restituisco un geojson contenente per ogni sito TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFGZZCWUGr7M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as geopd\n",
    "from tqdm.auto import tqdm\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "\n",
    "#Assegna ad ogni sito tp,tn,fp,fn in base all'intersezione della forma predetta \n",
    "#con la forma originale\n",
    "def test_for_intersection(site_id,path,verbose=False):\n",
    "    ### get shape for the original site\n",
    "    sites = geopd.read_file(PATH_SHAPE).to_crs(\"EPSG:3857\") # project to web-mercator\n",
    "    a = sites[sites.entry_id == site_id][[\"entry_id\",\"geometry\"]]\n",
    "    \n",
    "    if verbose: print('Loading Contours')\n",
    "    b = geopd.read_file(path+site_id+\".shp\").set_crs(\"EPSG:3857\")\n",
    "    b.geometry = b.geometry.convex_hull\n",
    "    b[\"entry_id\"] = \"pred\"\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"number of features:\",len(b))\n",
    "        print(b)\n",
    "        \n",
    "    if len(b) > 1: # if there is more than one shape\n",
    "        b[\"geometry\"] = MultiPolygon([feature for feature in b[\"geometry\"]])\n",
    "        b = b[:1].copy()\n",
    "        if verbose: \n",
    "            print(b)\n",
    "            b.plot()\n",
    "    # negs should have no geometry\n",
    "    if len(b) == 0 and site_id.startswith(\"neg\"):\n",
    "        if verbose: print(\"good neg\")\n",
    "        return \"TN\",site_id,None\n",
    "    \n",
    "    # if neg has geometry then FP\n",
    "    elif len(b) > 0 and site_id.startswith(\"neg\"):\n",
    "        if verbose: print(\"false positive\")\n",
    "        return \"FP\",site_id,b.iloc[0][\"geometry\"] #False\n",
    "    \n",
    "    # if no geometry and not neg then FN. use a.geometry for use in QGIS\n",
    "    elif len(b) == 0 and not site_id.startswith(\"neg\"):\n",
    "        if verbose: print(\"false negative\")\n",
    "        return \"FN\",site_id, a.iloc[0][\"geometry\"]\n",
    "    \n",
    "    # compute intersection\n",
    "    if ~b.iloc[0].geometry.is_valid:\n",
    "        b.geometry = b.geometry.buffer(0)\n",
    "    intersects = a.iloc[0][\"geometry\"].intersects(b.iloc[0][\"geometry\"])\n",
    "    if verbose:\n",
    "        c = pd.concat([a,b])\n",
    "        # print(c)\n",
    "        c.plot(column=\"entry_id\",legend=True,figsize=(5,5),cmap=\"Set3\")\n",
    "        print(\"INTERSECTION: \", intersects)\n",
    "        \n",
    "    if intersects: # right geometry\n",
    "        return \"TP\",site_id,b.iloc[0][\"geometry\"]\n",
    "    else: # wrong geometry\n",
    "        return \"FP\",site_id,b.iloc[0][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqIYsriiHX9B"
   },
   "outputs": [],
   "source": [
    "import geopandas as geopd\n",
    "#prende in input gli shapefile dei siti e ritorna un geojson \n",
    "#contenente id,nome sito, geometria e valore tra tp,tn,fp,fn\n",
    "def convertShapeToGeojson(testset,path_in,path_out):\n",
    " res = {\"index\":[],\"entry_id\":[],\"geometry\":[],\"cat\":[],}\n",
    " indice=0\n",
    " for sito in testset:\n",
    "    cat,eid,geom = test_for_intersection(sito,path_in,verbose=False)\n",
    "    res[\"index\"].append(indice)\n",
    "    res[\"entry_id\"].append(eid)\n",
    "    res[\"geometry\"].append(geom)\n",
    "    res[\"cat\"].append(cat)\n",
    "    indice+=1\n",
    " res_df = geopd.GeoDataFrame(res)\n",
    " res_df = res_df.set_index(\"index\")\n",
    " res_df.to_file(path_out, driver='GeoJSON',crs=\"EPSG:3857\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmUHRD95J-ro"
   },
   "source": [
    "###3.6 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uB86uU4ZehpX"
   },
   "outputs": [],
   "source": [
    "modelli_da_comparare=['bing+corona_2k_filtrato','bing_2k_filtrato']\n",
    "PATH_SHAPE='/shapefiles/site_shape/vw_site_survey_poly.shp'\n",
    "for modello in modelli_da_comparare:\n",
    "  PATH_OUTPUT='output/'+modello+'/'\n",
    "  set_config(modello)\n",
    "  name_ckpt = os.listdir(PATH_LOG+'lightning_logs/'+modello+'/checkpoints/')[0]\n",
    "  model= ArcheoModel.load_from_checkpoint(arch=config[\"arch\"],\n",
    "                 encoder_name=config[\"encoder\"], \n",
    "                 encoder_weights=config[\"weights\"],\n",
    "                 in_channels=config['in_channels'], out_classes=1,\n",
    "                 checkpoint_path=PATH_LOG+'lightning_logs/'+modello+'/checkpoints/'+name_ckpt)\n",
    "  random.seed(config[\"random_seed\"])\n",
    "  np.random.seed(config[\"random_seed\"])\n",
    "  torch.manual_seed(config[\"random_seed\"])\n",
    "  test_images_filenames,name2tras=crea_trasformazione('trasformazioni_modello.csv')\n",
    "  images_directory=config[\"dataset_path\"]+'/train/sites'\n",
    "  masks_directory=config[\"dataset_path\"]+'/train/masks'\n",
    "  test_dataset = ArcheoDatasetModify(test_images_filenames, \n",
    "                                     images_directory, \n",
    "                                     masks_directory)\n",
    "  test_loader = DataLoader(test_dataset,batch_size=config[\"batch_size\"],\n",
    "                           shuffle=False,drop_last=False,num_workers=0,)\n",
    "\n",
    "  print(len(test_images_filenames))\n",
    "  it = iter(test_loader)\n",
    "\n",
    "\n",
    "  for j in range(len(it)):\n",
    "    batch = next(it)\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      logits = model(batch[0])\n",
    "    pr_masks = logits.sigmoid()\n",
    "    stampa_predizioni_cutoff(batch,pr_masks,j,modello,0.2)\n",
    "    stampa_predizioni_cutoff(batch,pr_masks,j,modello,0.5)\n",
    "\n",
    "  convertImageToTif(modello,0.2,PATH_OUTPUT+'pred_siti_tronc0.2/',\n",
    "                    PATH_OUTPUT+'pred_siti_centro_tronc0.2/',PATH_OUTPUT+'tif_0.2/')\n",
    "  convertImageToTif(modello,0.5,PATH_OUTPUT+'pred_siti_tronc0.5/',\n",
    "                    PATH_OUTPUT+'pred_siti_centro_tronc0.5/',PATH_OUTPUT+'tif_0.5/')\n",
    "\n",
    "  convertTifToShape(PATH_OUTPUT+'tif_0.2/',PATH_OUTPUT+'shape_0.2/')\n",
    "  convertTifToShape(PATH_OUTPUT+'tif_0.5/',PATH_OUTPUT+'shape_0.5/')\n",
    "\n",
    "  convertShapeToGeojson(test_images_filenames,PATH_OUTPUT+'shape_0.2/',PATH_OUTPUT+'preds02.geojson')\n",
    "  convertShapeToGeojson(test_images_filenames,PATH_OUTPUT+'shape_0.5/',PATH_OUTPUT+'preds05.geojson')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJaH3-WzNyMt"
   },
   "source": [
    "## 4 Analisi Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_0SyTrCU8Uj"
   },
   "source": [
    "### 4.1 Calcolo della matrice di confusione, in modo automatico e adattato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9g4FjnPJJaO"
   },
   "outputs": [],
   "source": [
    "def calcola_matrix(preds):\n",
    "  tp= preds[preds.cat == \"TP\"]['entry_id'].shape[0]\n",
    "  tn= preds[preds.cat == \"TN\"]['entry_id'].shape[0]\n",
    "  fp= preds[preds.cat == \"FP\"]['entry_id'].shape[0]\n",
    "  fn= preds[preds.cat == \"FN\"]['entry_id'].shape[0]\n",
    "\n",
    "  matrix=[tp,tn,fp,fn]\n",
    "\n",
    "  # sono siti non visibili, quindi classificati erroneamente come tn\n",
    "  fn2tn = preds[(preds.cat == \"FN\")&(preds.correction == \"TN\")]['entry_id'].shape[0]\n",
    "\n",
    "  sites_inside_ot=preds[(preds.notes.str.contains('INSIDE OTHER',na=False)) ].shape[0]\n",
    "\n",
    "  #siti non visibili e il modello ne trova un altro esistente\n",
    "  fp2tp = preds[\n",
    "    (preds.cat == \"FP\")& ((preds.notes.str.contains('NV',na=False))| \n",
    "                        (preds.notes.str.contains('NOT VISIBLE',na=False)) |\n",
    "                        (preds.notes.str.contains('NOT VISIBILE',na=False)) |\n",
    "                        (preds.entry_id.str.contains('neg',na=False)))&\n",
    "                        (preds.notes.str.contains('INSIDE OTHER',na=False))].shape[0]\n",
    "\n",
    "  #siti visibili e il modello ne trova un altro esistente \n",
    "  fp2fn=sites_inside_ot-fp2tp\n",
    "\n",
    "  tn_a = tn + fn2tn + fp2tp\n",
    "  fn_a = fn - fn2tn + fp2fn\n",
    "  tp_a = tp + (fp2tp+fp2fn)\n",
    "  fp_a = fp - (fp2tp+fp2fn)\n",
    "\n",
    "  matrix_adj=[tp_a,tn_a,fp_a,fn_a]\n",
    "  \n",
    "  return(matrix,matrix_adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_DrUrJ0ZsGy"
   },
   "outputs": [],
   "source": [
    "#mi stampo le statistiche in termini di valori della matrice di confusione oltre a\n",
    "# precisione,recall.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def stampa_stats(cm,cm_adj,bing):\n",
    "  print('--------------------------------------------')\n",
    "  if(bing):\n",
    "    print(\"Stats Modello Bing 2k filtrato:\")\n",
    "  else:\n",
    "    print(\"Stats Modello Bing + Corona 2k filtrato:\")\n",
    "\n",
    "  print('---')\n",
    "\n",
    "  print(\"Valutazione automatica:\")\n",
    "  print(\"TP: \"+str(cm[0])+\" TN: \"+str(cm[1])+\" FP: \"+str(cm[2])+\" FN: \"+str(cm[3]))\n",
    "  print(\"Accuracy: \",round((cm[0]+cm[1])/(cm[0]+cm[1]+cm[2]+cm[3]),4))\n",
    "  print(\"Recall: \",round(cm[0]/(cm[0]+cm[3]),4))\n",
    "\n",
    "  print('---')\n",
    "\n",
    "  print(\"Valutazione manuale:\")\n",
    "  print(\"TP: \"+str(cm_adj[0])+\" TN: \"+str(cm_adj[1])+\" FP: \"+str(cm_adj[2])+\" FN: \"+str(cm_adj[3]))\n",
    "  print(\"Accuracy: \",round((cm_adj[0]+cm_adj[1])/(cm_adj[0]+cm_adj[1]+cm_adj[2]+cm_adj[3]),4))\n",
    "  print(\"Recall: \",round(cm_adj[0]/(cm_adj[0]+cm_adj[3]),4))\n",
    "\n",
    "  print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_GDMmUXk-q5",
    "outputId": "391f0ee3-9a46-490b-ff09-fa8f7a7c5a1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Stats Modello Bing 2k filtrato:\n",
      "---\n",
      "Valutazione automatica:\n",
      "TP: 228 TN: 98 FP: 70 FN: 125\n",
      "Accuracy:  0.6257\n",
      "Recall:  0.6459\n",
      "---\n",
      "Valutazione manuale:\n",
      "TP: 258 TN: 185 FP: 40 FN: 68\n",
      "Accuracy:  0.804\n",
      "Recall:  0.7914\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "Stats Modello Bing + Corona 2k filtrato:\n",
      "---\n",
      "Valutazione automatica:\n",
      "TP: 209 TN: 104 FP: 57 FN: 151\n",
      "Accuracy:  0.6008\n",
      "Recall:  0.5806\n",
      "---\n",
      "Valutazione manuale:\n",
      "TP: 239 TN: 197 FP: 27 FN: 88\n",
      "Accuracy:  0.7913\n",
      "Recall:  0.7309\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import geopandas as geopd\n",
    "PATH_GEOJSON_BING='/output/bing_2k_filtrato/preds05.geojson'\n",
    "PATH_GEOJSON_CORONA='/output/bing+corona_2k_filtrato/preds05.geojson'\n",
    "preds_bing = geopd.read_file(PATH_GEOJSON_BING).sort_values(\"index\").reset_index(drop=True)\n",
    "preds_corona = geopd.read_file(PATH_GEOJSON_CORONA).sort_values(\"index\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "cm_bing,cm_bing_adj=calcola_matrix(preds_bing)\n",
    "stampa_stats(cm_bing,cm_bing_adj,bing=True)\n",
    "\n",
    "\n",
    "cm_corona,cm_corona_adj=calcola_matrix(preds_corona)\n",
    "stampa_stats(cm_corona,cm_corona_adj,bing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OVuUTDUGFpo"
   },
   "source": [
    "##5 Area di selezione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckmrJv7FV3jQ"
   },
   "source": [
    "###5.1 Crea le predizioni per ogni tessera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAwuOOJuMSdR"
   },
   "outputs": [],
   "source": [
    "PATH_MAYSAN_DATASET='/datasets/maysan_sel_tile/'\n",
    "PATH_MAYSAN_OUTPUT='/output/maysan_sel_area/'\n",
    "PATH_MODEL_USED=PATH_LOG+'lightning_logs/bing+corona_2k_filtrato/checkpoints/epoch=19-step=2600.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ymw6DzvxvQXH"
   },
   "outputs": [],
   "source": [
    "def stampa_predizioni(ibatch,ipr_masks_11,num,patout):\n",
    "  for i,(image, gt_mask, masks_11, fn) in enumerate(zip(ibatch[0], ibatch[1], ipr_masks_11, ibatch[2])):\n",
    "\n",
    "    masks_11 = masks_11.numpy().squeeze()\n",
    "    plt.imsave(patout+'pred_magma_v1/'+ fn[:-4]+'.png',masks_11,cmap=\"magma\", vmin=0.0, vmax=1.0)\n",
    "    plt.imsave(patout+'pred_grey_v1/'+ fn[:-4]+'.png',masks_11,cmap=\"Greys\", vmin=0.0, vmax=1.0)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0agFYumlGUnM"
   },
   "outputs": [],
   "source": [
    "filenames_test_maysan = np.asarray(list(sorted(os.listdir(PATH_MAYSAN_DATASET+'sites'))))\n",
    "config['corona_path']=PATH_MAYSAN_DATASET+'corona_v1/'\n",
    "transform_maysan=A.Compose([A.Resize(512, 512),A.pytorch.ToTensorV2(),],\n",
    "                           additional_targets={'image_corona': 'image'})\n",
    "test_dataset_maysan = ArcheoDataset(filenames_test_maysan, PATH_MAYSAN_DATASET+'sites/', \n",
    "                                    PATH_MAYSAN_DATASET+'masks/', transform=transform_maysan)\n",
    "test_loader_maysan = DataLoader(test_dataset_maysan,batch_size=config[\"batch_size\"], \n",
    "                                shuffle=False,drop_last=False,num_workers=0,)\n",
    "model = ArcheoModel.load_from_checkpoint(arch=config[\"arch\"],\n",
    "                 encoder_name=config[\"encoder\"], \n",
    "                 encoder_weights=config[\"weights\"],\n",
    "                 in_channels=6, out_classes=1,checkpoint_path=PATH_MODEL_USED)\n",
    "\n",
    "random.seed(config[\"random_seed\"])\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "torch.manual_seed(config[\"random_seed\"])\n",
    "\n",
    "it_maysan=iter(test_loader_maysan)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "for i in range(len(it_maysan)):\n",
    "  batch = next(it_maysan)\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    logits = model(batch[0])\n",
    "  pr_masks = logits.sigmoid()\n",
    "  stampa_predizioni(batch,pr_masks,i,PATH_MAYSAN_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjqZMMcmWAnx"
   },
   "source": [
    "###5.2 Assembla l'immagine totale dell'area di selezione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq2DYnpepMBm"
   },
   "outputs": [],
   "source": [
    "def crea_righe(imm_size,l_x,l_y,fn,resize=False,num_res=256,format=\".png\"):\n",
    "  righe=[]\n",
    "  altezza=1\n",
    "  if(resize==True):\n",
    "    image_tot = Image.open(fn+\"1\"+format).resize((num_res,num_res))\n",
    "  else:\n",
    "    image_tot = Image.open(fn+\"1\"+format)\n",
    "  for i in range(2,l_x*l_y+1):\n",
    "    if((i-1)%l_x==0):\n",
    "      if(resize==True):\n",
    "        image_tot=Image.open(fn+str(i)+format).resize((num_res,num_res))\n",
    "      else:\n",
    "        image_tot=Image.open(fn+str(i)+format)\n",
    "    else:\n",
    "      if(resize==True):\n",
    "        image_open=Image.open(fn+str(i)+format).resize((num_res,num_res))\n",
    "      else:\n",
    "        image_open=Image.open(fn+str(i)+format)\n",
    "      new_image = Image.new('RGB',(imm_size*l_x, imm_size))\n",
    "      new_image.paste(image_tot,(0,0))\n",
    "      posizione=i%l_x\n",
    "      if(posizione==0):\n",
    "        posizione=l_x\n",
    "      new_image.paste(image_open,(imm_size*(posizione-1),((altezza-1)*image_open.size[1])))\n",
    "      image_tot=new_image\n",
    "      if(posizione==l_x):righe.append(image_tot)\n",
    "  return righe\n",
    "\n",
    "def crea_immagine(righ,dim,l_x,l_y,nome):\n",
    "  imm_totale = Image.new('RGB',(l_x*dim, l_y*dim))\n",
    "  x=-dim\n",
    "  for i in reversed(range(len(righ))):\n",
    "    x+=dim\n",
    "    imm_totale.paste(righ[i],(0,x))\n",
    "  imm_totale.save(PATH_MAYSAN_OUTPUT+nome+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmCZ89aHsdBQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "f = open('coor_maysan_1000.json')\n",
    "data = json.load(f)\n",
    "spost=data[1]['cx']-data[0]['cx']\n",
    "ovest=int(data[0]['cx']-spost/2)\n",
    "sud=int(data[0]['cy']-spost/2)\n",
    "nord=int(data[len(data)-1]['cy']+(spost/2))\n",
    "num_righe=int((nord-sud)/spost)\n",
    "num_colonne=int(len(data)/num_righe)\n",
    "est=int(data[num_colonne-1]['cx']+spost/2)\n",
    "\n",
    "\n",
    "righe=crea_righe(512,num_colonne,num_righe,PATH_MAYSAN_OUTPUT+'pred_magma_v1/')\n",
    "righe1=crea_righe(512,num_colonne,num_righe,PATH_MAYSAN_OUTPUT+'pred_grey_v1/')\n",
    "crea_immagine(righe,512,num_colonne,num_righe,\"imm_tot_magma_v1\")\n",
    "crea_immagine(righe1,512,num_colonne,num_righe,\"imm_tot_grey_v1\")\n",
    "createTif(ovest,sud,est,nord,PATH_MAYSAN_OUTPUT+\"imm_tot_magma_v1.jpg\",\n",
    "          PATH_MAYSAN_OUTPUT+\"imm_tot_magma_v1.tif\")\n",
    "createTif(ovest,sud,est,nord,PATH_MAYSAN_OUTPUT+\"imm_tot_grey_v1.jpg\",\n",
    "          PATH_MAYSAN_OUTPUT+\"imm_tot_grey_v1.tif\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
